{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 02 – ERA5 Weather Download (Raw Data)\n",
    "\n",
    "This notebook downloads ERA5 single-level weather data from the Copernicus Climate Data Store (CDS)  \n",
    "for multiple bidding zones and stores the raw NetCDF files under `data/raw/weather`.\n",
    "\n",
    "The data will later be merged with price data for further analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Objectives\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "- Define a project-relative output directory for ERA5 raw weather data\n",
    "- Configure geographic areas (bounding boxes) for the bidding zones (ES, DK1, NO2, NO4)\n",
    "- Define the time range (years, months) and ERA5 variables we want to download\n",
    "- Initialize the CDS API client using credentials from `config/secrets.env`\n",
    "- Implement a robust download function with:\n",
    "  - Progress logging\n",
    "  - Simple validation that the NetCDF file is readable\n",
    "  - Per-file timing\n",
    "- Run an execution loop over all zones, years and months with:\n",
    "  - Global progress counters\n",
    "  - Total runtime measurement\n",
    "- Add checks to:\n",
    "  - List downloaded files and summarize them by zone and year\n",
    "  - Detect missing `(zone, year, month)` combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Setup paths, environment, and imports\n",
    "\n",
    "We assume the following project structure:\n",
    "\n",
    "- `<project_root>/notebooks/02_weather_era5.ipynb`   (this notebook)\n",
    "- `<project_root>/config/secrets.env`               (CDS credentials)\n",
    "- `<project_root>/data/raw/weather`                 (output folder for ERA5 NetCDF files)\n",
    "\n",
    "The notebook will:\n",
    "- Resolve `project_root` relative to the current working directory (the `notebooks` folder)\n",
    "- Ensure that `data/raw/weather` exists\n",
    "- Load the `secrets.env` file which must define `CDS_URL` and `CDS_KEY`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import zipfile\n",
    "\n",
    "import cdsapi\n",
    "import netCDF4\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. Setup paths (relative to the notebooks directory)\n",
    "current_dir = Path.cwd()\n",
    "project_dir = current_dir.parent.parent  # up from /notebooks to project root\n",
    "secrets_path = project_dir / \"config\" / \"secrets.env\"\n",
    "\n",
    "# 2. Define output directory for raw weather data\n",
    "output_dir = project_dir / \"data\" / \"raw\" / \"weather\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Current working directory: {current_dir}\")\n",
    "print(f\"Project directory        : {project_dir}\")\n",
    "print(f\"Secrets file             : {secrets_path}\")\n",
    "print(f\"Weather output directory : {output_dir.resolve()}\")\n",
    "\n",
    "# 3. Load secrets (CDS credentials)\n",
    "if not secrets_path.exists():\n",
    "    print(\"Warning: secrets.env file not found at:\", secrets_path)\n",
    "else:\n",
    "    load_dotenv(secrets_path)\n",
    "    print(\"Loaded environment variables from secrets.env\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. ERA5 configuration\n",
    "\n",
    "Here we define:\n",
    "\n",
    "- `AREAS`: geographic bounding boxes for each bidding zone  \n",
    "  Format: `north, west, south, east` (ERA5 / CDS expected format)\n",
    "- `YEARS`: years to download\n",
    "- `MONTHS`: months to download (two-digit strings `\"01\"`, `\"02\"`, ..., `\"12\"`)\n",
    "- `VARIABLES`: ERA5 variables required for later analysis\n",
    "\n",
    "The bounding boxes have been derived using `https://boundingbox.klokantech.com`  \n",
    "and then converted into the `[North, West, South, East]` format expected by ERA5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidding zone areas in ERA5 format: [North, West, South, East]\n",
    "# Derived from boundingbox.klokantech.com (minLon, minLat, maxLon, maxLat)\n",
    "# and converted to [maxLat, minLon, minLat, maxLon].\n",
    "\n",
    "AREAS = {\n",
    "    \"DK1\": {\n",
    "        \"north\": 57.846,\n",
    "        \"west\": 7.8714,\n",
    "        \"south\": 54.7545,\n",
    "        \"east\": 11.0739,\n",
    "    },\n",
    "    \"ES\": {\n",
    "        \"north\": 43.733,\n",
    "        \"west\": -9.5816,\n",
    "        \"south\": 36.0242,\n",
    "        \"east\": 3.4922,\n",
    "    },\n",
    "    \"NO2\": {\n",
    "        \"north\": 59.361,\n",
    "        \"west\": 5.3171,\n",
    "        \"south\": 57.9584,\n",
    "        \"east\": 9.8517,\n",
    "    },\n",
    "    #\"NO4\": {\n",
    "    #    \"north\": 71.09,\n",
    "    #    \"west\": 12.55,\n",
    "    #    \"south\": 66.3,\n",
    "    #    \"east\": 30.85,\n",
    "    #},\n",
    "}\n",
    "\n",
    "# Time configuration\n",
    "YEARS = [str(year) for year in range(2023, 2026)]  \n",
    "MONTHS = [f\"{month:02d}\" for month in range(1, 13)]\n",
    "\n",
    "# ERA5 variables\n",
    "VARIABLES = [\n",
    "    \"2m_temperature\",\n",
    "    \"total_precipitation\",\n",
    "    \"10m_u_component_of_wind\",\n",
    "    \"10m_v_component_of_wind\",\n",
    "    \"surface_solar_radiation_downwards\",\n",
    "]\n",
    "\n",
    "print(\"Configured zones: \", list(AREAS.keys()))\n",
    "print(\"Years           : \", YEARS)\n",
    "print(\"Months          : \", MONTHS)\n",
    "print(\"Variables       : \", VARIABLES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Initialize CDS API client and connectivity check\n",
    "\n",
    "We initialize the CDS client using:\n",
    "\n",
    "- `CDS_URL` and `CDS_KEY` from `config/secrets.env`\n",
    "\n",
    "Then we perform a lightweight HTTP GET request to the CDS API endpoint to verify basic network connectivity.  \n",
    "This does **not** download data, it only checks if the endpoint is reachable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CDS client\n",
    "try:\n",
    "    cds_url = os.getenv(\"CDS_URL\")\n",
    "    cds_key = os.getenv(\"CDS_KEY\")\n",
    "\n",
    "    if not cds_url or not cds_key:\n",
    "        raise ValueError(\"Missing CDS_URL or CDS_KEY in environment variables.\")\n",
    "\n",
    "    client = cdsapi.Client(url=cds_url, key=cds_key)\n",
    "    print(\"CDS API Client initialized successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    client = None\n",
    "    print(f\"Initialization failed: {e}\")\n",
    "\n",
    "\n",
    "def check_cds_connection():\n",
    "    \"\"\"\n",
    "    Simple connectivity check for the CDS API endpoint.\n",
    "    This does not use authentication, it only checks if the endpoint is reachable.\n",
    "    \"\"\"\n",
    "    url = \"https://cds.climate.copernicus.eu/api/v2\"\n",
    "    print(f\"Checking basic connectivity to CDS API: {url}\")\n",
    "    start = time.time()\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  HTTP status: {response.status_code}\")\n",
    "        print(f\"  Elapsed    : {elapsed:.2f} seconds\")\n",
    "        if 200 <= response.status_code < 500:\n",
    "            print(\"  Network connectivity looks OK (request reached CDS).\")\n",
    "        else:\n",
    "            print(\"  Unexpected status code. Check credentials or CDS status page if downloads fail.\")\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  Connection failed after {elapsed:.2f} seconds:\")\n",
    "        print(f\"  {e}\")\n",
    "\n",
    "\n",
    "# Optional: run connectivity check\n",
    "if client is not None:\n",
    "    check_cds_connection()\n",
    "else:\n",
    "    print(\"Client is not initialized, skipping connectivity check.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. ERA5 download helper function\n",
    "\n",
    "The function `download_era5_month`:\n",
    "\n",
    "- Builds the output filename: `era5_<ZONE>_<YEAR>_<MONTH>.nc`\n",
    "- Skips download if the file already exists (idempotent behavior)\n",
    "- Sends the request to the ERA5 dataset `reanalysis-era5-single-levels`\n",
    "- Measures per-file runtime\n",
    "- Validates the resulting NetCDF file (can it be opened, and variables listed?)\n",
    "- Returns a status string: `\"downloaded\"`, `\"skipped\"`, or `\"failed\"`\n",
    "\n",
    "This function is used in the main execution loop below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. ERA5 download helper function\n",
    "\n",
    "DATASET = \"reanalysis-era5-single-levels\"\n",
    "\n",
    "\n",
    "def _maybe_unzip_era5_file(file_path: Path) -> Path:\n",
    "    \"\"\"\n",
    "    If file_path is actually a ZIP archive (starts with 'PK'),\n",
    "    extract the contained NetCDF file(s) and return the primary .nc path.\n",
    "\n",
    "    Behaviour:\n",
    "    - If header is not ZIP -> return file_path unchanged\n",
    "    - If ZIP:\n",
    "        - Rename file_path to file_path + '.zip'\n",
    "        - Extract contents to output_dir\n",
    "        - Rename NetCDFs to:\n",
    "            <stem>_instant.nc or <stem>_accum.nc depending on the name\n",
    "        - If an '_instant.nc' exists, return that for validation,\n",
    "          otherwise return the first .nc.\n",
    "    \"\"\"\n",
    "    if not file_path.exists():\n",
    "        return file_path\n",
    "\n",
    "    # Check ZIP signature\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        sig = f.read(2)\n",
    "\n",
    "    if sig != b\"PK\":\n",
    "        # Not a ZIP archive, nothing to do\n",
    "        return file_path\n",
    "\n",
    "    print(\"  Detected ZIP archive instead of plain NetCDF. Unzipping...\")\n",
    "\n",
    "    stem = file_path.stem  # e.g. 'era5_DK1_2023_01'\n",
    "    zip_path = file_path.with_suffix(file_path.suffix + \".zip\")\n",
    "    if zip_path.exists():\n",
    "        zip_path.unlink()\n",
    "    file_path.rename(zip_path)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        members = zf.namelist()\n",
    "        nc_members = [m for m in members if m.endswith(\".nc\")]\n",
    "\n",
    "        print(f\"  ZIP contents: {len(members)} files, {len(nc_members)} NetCDF file(s).\")\n",
    "        zf.extractall(path=output_dir)\n",
    "\n",
    "    if not nc_members:\n",
    "        print(\"  Warning: no .nc files found inside ZIP. Leaving ZIP on disk.\")\n",
    "        return zip_path\n",
    "\n",
    "    renamed_paths = []\n",
    "    instant_path = None\n",
    "\n",
    "    for nc_name in nc_members:\n",
    "        src = output_dir / nc_name\n",
    "\n",
    "        lower = nc_name.lower()\n",
    "        if \"instant\" in lower:\n",
    "            dst = output_dir / f\"{stem}_instant.nc\"\n",
    "        elif \"accum\" in lower:\n",
    "            dst = output_dir / f\"{stem}_accum.nc\"\n",
    "        else:\n",
    "            dst = output_dir / f\"{stem}_{nc_name.replace('/', '_')}\"\n",
    "\n",
    "        if dst.exists():\n",
    "            dst.unlink()\n",
    "        src.rename(dst)\n",
    "        renamed_paths.append(dst)\n",
    "\n",
    "        if \"instant\" in lower:\n",
    "            instant_path = dst\n",
    "\n",
    "    # Clean up ZIP\n",
    "    zip_path.unlink(missing_ok=True)\n",
    "\n",
    "    print(\"  Extracted NetCDF files:\")\n",
    "    for p in renamed_paths:\n",
    "        print(f\"    - {p.name}\")\n",
    "\n",
    "    if instant_path is not None:\n",
    "        print(f\"  Using instant file for validation: {instant_path.name}\")\n",
    "        return instant_path\n",
    "\n",
    "    # No explicit instant file: just use the first\n",
    "    first_nc = renamed_paths[0]\n",
    "    print(f\"  Using first NetCDF for validation: {first_nc.name}\")\n",
    "    return first_nc\n",
    "\n",
    "\n",
    "def download_era5_month(year: str, month: str, zone: str, coords: dict) -> str:\n",
    "    \"\"\"\n",
    "    Download one ERA5 month for a given zone, if needed.\n",
    "\n",
    "    Rules:\n",
    "    - If both <stem>_instant.nc and <stem>_accum.nc exist and are valid -> skip\n",
    "    - Else, download to <stem>.nc, unzip if necessary, create _instant/_accum, validate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : str   e.g. \"2023\"\n",
    "    month: str   e.g. \"01\"\n",
    "    zone : str   e.g. \"DK1\"\n",
    "    coords : dict with keys 'north', 'west', 'south', 'east'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    status : \"downloaded\", \"skipped\", or \"failed\"\n",
    "    \"\"\"\n",
    "    if client is None:\n",
    "        print(\"CDS client is not initialized. Cannot download.\")\n",
    "        return \"failed\"\n",
    "\n",
    "    stem = f\"era5_{zone}_{year}_{month}\"\n",
    "    instant_path = output_dir / f\"{stem}_instant.nc\"\n",
    "    accum_path = output_dir / f\"{stem}_accum.nc\"\n",
    "    base_nc_path = output_dir / f\"{stem}.nc\"\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 1) Check if we already have valid instant + accum data\n",
    "    # ------------------------------------------------------\n",
    "    if instant_path.exists() and accum_path.exists():\n",
    "        print(f\"\\nExisting files found for {zone} {year}-{month}:\")\n",
    "        print(f\"  {instant_path.name}\")\n",
    "        print(f\"  {accum_path.name}\")\n",
    "\n",
    "        try:\n",
    "            # Quick validation: can both files be opened as NetCDF?\n",
    "            with netCDF4.Dataset(instant_path, \"r\") as nc_i:\n",
    "                _ = list(nc_i.variables.keys())\n",
    "            with netCDF4.Dataset(accum_path, \"r\") as nc_a:\n",
    "                _ = list(nc_a.variables.keys())\n",
    "\n",
    "            print(\"  Both instant and accum files are valid NetCDF. Skipping download.\")\n",
    "            return \"skipped\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"  Existing files are not valid NetCDF, will re-download.\")\n",
    "            print(f\"  Reason: {e}\")\n",
    "            # Remove broken files and continue to fresh download\n",
    "            if instant_path.exists():\n",
    "                instant_path.unlink()\n",
    "            if accum_path.exists():\n",
    "                accum_path.unlink()\n",
    "            if base_nc_path.exists():\n",
    "                base_nc_path.unlink()\n",
    "\n",
    "    # ------------------------------------------------------\n",
    "    # 2) No valid instant/accum files -> download (or reuse .nc if present)\n",
    "    # ------------------------------------------------------\n",
    "    # If a leftover base_nc_path exists (from interrupted run), try to unzip/use it\n",
    "    if base_nc_path.exists():\n",
    "        print(f\"\\nFound leftover base file: {base_nc_path.name}\")\n",
    "        nc_path = _maybe_unzip_era5_file(base_nc_path)\n",
    "        # After this, instant/accum files should exist; the next section will validate them\n",
    "\n",
    "    print(f\"\\nRequesting file: {stem}.nc\")\n",
    "    print(f\"  Year: {year}, Month: {month}, Zone: {zone}\")\n",
    "    print(f\"  Area (N,W,S,E): [{coords['north']}, {coords['west']}, {coords['south']}, {coords['east']}]\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Build CDS request payload\n",
    "    request = {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"format\": \"netcdf\",  # CDS may still return a ZIP with NetCDF inside\n",
    "        \"variable\": VARIABLES,\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"day\": [f\"{day:02d}\" for day in range(1, 32)],     # 01-31\n",
    "        \"time\": [f\"{hour:02d}:00\" for hour in range(24)],  # 00:00-23:00\n",
    "        \"area\": [coords[\"north\"], coords[\"west\"], coords[\"south\"], coords[\"east\"]],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Submit request\n",
    "        print(\"  Submitting request to CDS API...\")\n",
    "        client.retrieve(DATASET, request, target=str(base_nc_path))\n",
    "        print(\"  Download finished, checking file size...\")\n",
    "\n",
    "        if not base_nc_path.exists():\n",
    "            print(\"  Error: base .nc file does not exist after download.\")\n",
    "            return \"failed\"\n",
    "\n",
    "        size_bytes = base_nc_path.stat().st_size\n",
    "        print(f\"  File size: {size_bytes} bytes (~{size_bytes / (1024*1024):.2f} MB)\")\n",
    "\n",
    "        # Unzip if needed and create _instant/_accum files\n",
    "        _ = _maybe_unzip_era5_file(base_nc_path)\n",
    "\n",
    "        # After unzipping, we expect instant/accum files\n",
    "        if not (instant_path.exists() and accum_path.exists()):\n",
    "            print(\"  Error: instant/accum files not found after unzipping.\")\n",
    "            return \"failed\"\n",
    "\n",
    "        # Validate NetCDF files\n",
    "        try:\n",
    "            with netCDF4.Dataset(instant_path, \"r\") as nc_i:\n",
    "                vars_instant = list(nc_i.variables.keys())\n",
    "            with netCDF4.Dataset(accum_path, \"r\") as nc_a:\n",
    "                vars_accum = list(nc_a.variables.keys())\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"  Success: {instant_path.name}, {accum_path.name}\")\n",
    "            print(f\"  Instant vars: {vars_instant}\")\n",
    "            print(f\"  Accum vars  : {vars_accum}\")\n",
    "            print(f\"  Elapsed time for this month: {elapsed:.1f} seconds\")\n",
    "            return \"downloaded\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"  Validation error for instant/accum files:\")\n",
    "            print(f\"  {e}\")\n",
    "            return \"failed\"\n",
    "\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  Failed to download {stem}.nc after {elapsed:.1f} seconds:\")\n",
    "        print(f\"  {e}\")\n",
    "        return \"failed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. Execution loop with progress and total timing\n",
    "\n",
    "This loop runs over all zones, years, and months:\n",
    "\n",
    "- Uses `download_era5_month` for each `(zone, year, month)` combination\n",
    "- Tracks how many files were:\n",
    "  - downloaded\n",
    "  - skipped (already existed)\n",
    "  - failed\n",
    "- Prints ongoing progress after every file\n",
    "- Measures and prints total runtime at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if client is None:\n",
    "    print(\"CDS client is not initialized. Skipping download loop.\")\n",
    "else:\n",
    "    print(\"Starting ERA5 download queue.\")\n",
    "    print(\"Note: This process may take time depending on the CDS queue.\")\n",
    "\n",
    "    total_files = len(AREAS) * len(YEARS) * len(MONTHS)\n",
    "    downloaded = 0\n",
    "    skipped = 0\n",
    "    failed = 0\n",
    "\n",
    "    overall_start = time.time()\n",
    "\n",
    "    for zone, coords in AREAS.items():\n",
    "        print(\"\\n======================================\")\n",
    "        print(f\"Processing Zone: {zone}\")\n",
    "        print(\"--------------------------------------\")\n",
    "\n",
    "        for year in YEARS:\n",
    "            for month in MONTHS:\n",
    "                status = download_era5_month(year, month, zone, coords)\n",
    "\n",
    "                if status == \"downloaded\":\n",
    "                    downloaded += 1\n",
    "                elif status == \"skipped\":\n",
    "                    skipped += 1\n",
    "                elif status == \"failed\":\n",
    "                    failed += 1\n",
    "\n",
    "                processed = downloaded + skipped + failed\n",
    "                print(\n",
    "                    f\"  Progress: {processed}/{total_files} \"\n",
    "                    f\"(downloaded={downloaded}, skipped={skipped}, failed={failed})\"\n",
    "                )\n",
    "\n",
    "    overall_elapsed = time.time() - overall_start\n",
    "    overall_minutes = overall_elapsed / 60.0\n",
    "\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"All downloads completed.\")\n",
    "    print(f\"Summary:\")\n",
    "    print(f\"  Downloaded: {downloaded}\")\n",
    "    print(f\"  Skipped   : {skipped}\")\n",
    "    print(f\"  Failed    : {failed}\")\n",
    "    print(f\"  Total     : {total_files}\")\n",
    "    print(f\"Total elapsed time: {overall_minutes:.1f} minutes ({overall_elapsed:.0f} seconds)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 7. Check downloaded ERA5 files\n",
    "\n",
    "This section provides a quick overview of all downloaded ERA5 NetCDF files in the  \n",
    "`data/raw/weather` directory.\n",
    "\n",
    "It shows:\n",
    "\n",
    "- The total number of files\n",
    "- The first few file names for a quick manual check\n",
    "- A summary by zone and year (number of months found)\n",
    "\n",
    "This allows us to verify that:\n",
    "\n",
    "- The download loop produced the expected coverage\n",
    "- Re-runs correctly skip already existing files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Weather directory: {output_dir.resolve()}\")\n",
    "\n",
    "# List all NetCDF files that follow the naming pattern:\n",
    "#   era5_<ZONE>_<YEAR>_<MONTH>_*.nc\n",
    "all_files = sorted(output_dir.glob(\"era5_*.nc\"))\n",
    "print(f\"Total ERA5 files found: {len(all_files)}\")\n",
    "\n",
    "if not all_files:\n",
    "    print(\"No files found. Check the download loop and paths.\")\n",
    "else:\n",
    "    print(\"\\nFirst 20 files (sorted):\")\n",
    "    for path in all_files[:20]:\n",
    "        print(\"  \", path.name)\n",
    "\n",
    "    # Parse file names and collect stats\n",
    "    # We want to count DISTINCT MONTHS per (zone, year),\n",
    "    # regardless of whether \"instant\" and \"accum\" both exist.\n",
    "    per_zone_year_months = defaultdict(set)\n",
    "    zones_seen = set()\n",
    "    years_seen = set()\n",
    "    parse_errors = []\n",
    "\n",
    "    for path in all_files:\n",
    "        # Expected pattern(s):\n",
    "        #   era5_ZONE_YEAR_MONTH_instant.nc\n",
    "        #   era5_ZONE_YEAR_MONTH_accum.nc\n",
    "        name = path.stem  # without .nc\n",
    "        parts = name.split(\"_\")\n",
    "        if len(parts) < 4 or parts[0] != \"era5\":\n",
    "            parse_errors.append(name)\n",
    "            continue\n",
    "\n",
    "        # Take only the first four parts: era5, ZONE, YEAR, MONTH\n",
    "        _, zone, year_str, month_str = parts[0:4]\n",
    "\n",
    "        zones_seen.add(zone)\n",
    "        years_seen.add(year_str)\n",
    "        per_zone_year_months[(zone, year_str)].add(month_str)\n",
    "\n",
    "    if parse_errors:\n",
    "        print(\"\\nWarning: Some files do not match the expected name pattern:\")\n",
    "        for n in parse_errors:\n",
    "            print(\"  \", n)\n",
    "\n",
    "    print(\"\\nZones present on disk:\", sorted(zones_seen))\n",
    "    print(\"Years present on disk:\", sorted(years_seen))\n",
    "\n",
    "    print(\"\\nSummary: number of DISTINCT months per (zone, year)\")\n",
    "    for (zone, year_str), month_set in sorted(per_zone_year_months.items()):\n",
    "        print(f\"  {zone} {year_str}: {len(month_set):2d} months\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 8. Check for missing zone–year–month combinations\n",
    "\n",
    "Here we compare:\n",
    "\n",
    "- All expected combinations of `AREAS × YEARS × MONTHS`\n",
    "- With all existing files on disk\n",
    "\n",
    "For each `(zone, year, month)` that is expected but **not** present as a file,  \n",
    "we report it as \"missing\".\n",
    "\n",
    "This is useful to:\n",
    "\n",
    "- Detect incomplete downloads\n",
    "- Decide whether we need to rerun the notebook only for certain months or zones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the expected set of (zone, year, month) combinations\n",
    "expected = set()\n",
    "for zone in AREAS.keys():\n",
    "    for year in YEARS:\n",
    "        for month in MONTHS:\n",
    "            expected.add((zone, str(year), str(month)))\n",
    "\n",
    "# Build the set of existing combinations from the file names\n",
    "existing = set()\n",
    "parse_errors = []\n",
    "\n",
    "for path in all_files:\n",
    "    name = path.stem  # e.g. \"era5_DK1_2023_01_instant\"\n",
    "    parts = name.split(\"_\")\n",
    "    if len(parts) < 4 or parts[0] != \"era5\":\n",
    "        parse_errors.append(name)\n",
    "        continue\n",
    "\n",
    "    # Ignore trailing parts like \"instant\"/\"accum\"\n",
    "    _, zone, year_str, month_str = parts[0:4]\n",
    "    existing.add((zone, year_str, month_str))\n",
    "\n",
    "missing = expected - existing\n",
    "\n",
    "print(f\"Expected combinations: {len(expected)}\")\n",
    "print(f\"Existing combinations: {len(existing)}\")\n",
    "print(f\"Missing combinations : {len(missing)}\")\n",
    "\n",
    "if parse_errors:\n",
    "    print(\"\\nWarning: some files could not be parsed into (zone, year, month):\")\n",
    "    for n in parse_errors:\n",
    "        print(\"  \", n)\n",
    "\n",
    "if missing:\n",
    "    print(\"\\nMissing (zone, year, month) combinations:\")\n",
    "    for zone, year_str, month_str in sorted(missing):\n",
    "        print(f\"  Zone={zone}, Year={year_str}, Month={month_str}\")\n",
    "else:\n",
    "    print(\"\\nNo missing combinations. All expected files are present.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 9. Notes and next steps\n",
    "\n",
    "- The downloaded ERA5 NetCDF files are now available in `data/raw/weather`\n",
    "- Each file contains one month of hourly data for one bidding zone\n",
    "- The next notebook will:\n",
    "  - Load these NetCDF files\n",
    "  - Aggregate and transform the variables as needed\n",
    "  - Merge them with price data for the same zones and timestamps\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
