{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08494d99",
   "metadata": {},
   "source": [
    "# Price Data Preprocessing\n",
    "\n",
    "**Objectives:**\n",
    "1.  **Clean:** Resample to hourly, fill gaps (Forward Fill), and handle outliers.\n",
    "2.  **Timezone:** Convert to Europe/Vienna.\n",
    "3.  **Reshape:** Transform into a **Wide Format** (Date x 24 Hours).\n",
    "4.  **DST Handling:** Merge duplicates and fill missing spring-forward hours.\n",
    "5.  **Formatting:** Ensure all values are Floats rounded to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5197af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cleaning, Reshaping & Rounding Pipeline...\n",
      "\n",
      "Processed ES:\n",
      "  Shape: (3651, 24) (Days x Hours)\n",
      "  Date Range: 2015-11-29 to 2025-11-26\n",
      "  Saved to: ES_processed.csv\n",
      "\n",
      "Processed NO2:\n",
      "  Shape: (3651, 24) (Days x Hours)\n",
      "  Date Range: 2015-11-29 to 2025-11-26\n",
      "  Saved to: NO2_processed.csv\n",
      "\n",
      "Processed NO4:\n",
      "  Shape: (3651, 24) (Days x Hours)\n",
      "  Date Range: 2015-11-29 to 2025-11-26\n",
      "  Saved to: NO4_processed.csv\n",
      "\n",
      "Processed DK1:\n",
      "  Shape: (3651, 24) (Days x Hours)\n",
      "  Date Range: 2015-11-29 to 2025-11-26\n",
      "  Saved to: DK1_processed.csv\n",
      "\n",
      "Preprocessing Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Setup Paths\n",
    "if 'DATA_DIR' not in locals():\n",
    "    DATA_DIR = Path('../../data')\n",
    "    \n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "PROCESSED_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Configuration\n",
    "ZONES = {\n",
    "    'ES': '10YES-REE------0',\n",
    "    'NO2': '10YNO-2--------T',\n",
    "    'NO4': '10YNO-4--------9',\n",
    "    'DK1': '10YDK-1--------W'\n",
    "}\n",
    "\n",
    "def process_and_reshape(zone_name):\n",
    "    input_path = RAW_DIR / f\"{zone_name}_raw.csv\"\n",
    "    if not input_path.exists():\n",
    "        print(f\"Skipping {zone_name}: File not found.\")\n",
    "        return\n",
    "\n",
    "    # --- Part 1: Standard Cleaning ---\n",
    "    \n",
    "    # Load & Sort\n",
    "    df = pd.read_csv(input_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "    df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "    # Resample to 1H (Mean of sub-hourly periods)\n",
    "    df_hourly = df.resample('1h').mean()\n",
    "\n",
    "    # Reindex (Create perfect timeline)\n",
    "    full_range = pd.date_range(\n",
    "        start=df_hourly.index.min(),\n",
    "        end=df_hourly.index.max(),\n",
    "        freq='h',\n",
    "        tz='UTC'\n",
    "    )\n",
    "    df_clean = df_hourly.reindex(full_range)\n",
    "\n",
    "    # Fill Gaps (Forward Fill)\n",
    "    df_clean['price_eur_mwh'] = df_clean['price_eur_mwh'].ffill().bfill()\n",
    "\n",
    "    # Convert Timezone (UTC -> Vienna)\n",
    "    df_clean.index = df_clean.index.tz_convert('Europe/Vienna')\n",
    "\n",
    "    # --- Part 2: Reshape to Wide Format ---\n",
    "    # Extract Date and Hour\n",
    "    df_clean['date'] = df_clean.index.date\n",
    "    df_clean['hour'] = df_clean.index.hour\n",
    "    \n",
    "    # Group by Date/Hour and take Mean to merge duplicate hours (Fall back)\n",
    "    df_grouped = df_clean.groupby(['date', 'hour'])['price_eur_mwh'].mean().reset_index()\n",
    "    \n",
    "    # Pivot: Index=Date, Columns=Hour, Values=Price\n",
    "    df_wide = df_grouped.pivot(index='date', columns='hour', values='price_eur_mwh')\n",
    "    \n",
    "    # Rename columns to h00, h01... h23\n",
    "    df_wide.columns = [f'h{h:02d}' for h in df_wide.columns]\n",
    "    \n",
    "    # Handle Missing DST Hours (Spring Forward)\n",
    "    if df_wide.isna().sum().sum() > 0:\n",
    "        df_wide = df_wide.ffill(axis=1).bfill(axis=1)\n",
    "\n",
    "    # --- Part 3: Final Formatting ---\n",
    "    # Ensure Floats and Round to 2 decimal places\n",
    "    df_wide = df_wide.astype(float).round(2)\n",
    "\n",
    "    # Save\n",
    "    output_path = PROCESSED_DIR / f\"{zone_name}_processed.csv\"\n",
    "    df_wide.to_csv(output_path)\n",
    "    \n",
    "    print(f\"Processed {zone_name}:\")\n",
    "    print(f\"  Shape: {df_wide.shape} (Days x Hours)\")\n",
    "    print(f\"  Date Range: {df_wide.index.min()} to {df_wide.index.max()}\")\n",
    "    print(f\"  Saved to: {output_path.name}\\n\")\n",
    "\n",
    "# Execute\n",
    "print(\"Starting Cleaning, Reshaping & Rounding Pipeline...\\n\")\n",
    "for zone in ZONES.keys():\n",
    "    process_and_reshape(zone)\n",
    "print(\"Preprocessing Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfcd8bf",
   "metadata": {},
   "source": [
    "## Outlier detection (tbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e98971c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49b1a484",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
