{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electricity Price Data Preprocessing for Clustering and ML\n",
    "\n",
    "**Input**: Raw hourly price data from 5 largest European zones\n",
    "\n",
    "**Output**: Clean 24-hour daily profiles ready for clustering and XGBoost modeling\n",
    "\n",
    "## Scientific Methodology\n",
    "\n",
    "Following validated preprocessing methods from peer-reviewed literature:\n",
    "\n",
    "1. **Outlier Detection**: Z-score (threshold=3) - Chikodili et al. (2021)\n",
    "2. **Missing Data**: Cubic spline interpolation - Moritz & Bartz-Beielstein (2017)\n",
    "3. **Normalization**: Min-max [0,1] and z-score - Faiq et al. (2024)\n",
    "4. **Framework**: Systematic preprocessing - Wang et al. (2023)\n",
    "\n",
    "## Zones Processed\n",
    "\n",
    "- **ES**: Spain\n",
    "- **PL**: Poland\n",
    "- **NO2**: Norway (Southwest, largest)\n",
    "- **SE3**: Sweden (Central, largest)\n",
    "- **DK1**: Denmark (West, largest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from scipy.stats import zscore\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory structure\n",
    "DATA_DIR = Path('data')\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "CLEAN_DIR = DATA_DIR / 'clean'\n",
    "PROFILES_DIR = DATA_DIR / 'daily_profiles'\n",
    "FIGURES_DIR = DATA_DIR / 'figures'\n",
    "\n",
    "CLEAN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "PROFILES_DIR.mkdir(exist_ok=True, parents=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Zone configuration (largest per country)\n",
    "ZONES = ['ES', 'PL', 'NO2', 'SE3', 'DK1']\n",
    "\n",
    "COUNTRY_MAP = {\n",
    "    'ES': 'Spain',\n",
    "    'PL': 'Poland',\n",
    "    'NO2': 'Norway',\n",
    "    # 'SE2': 'Sweden',  # Backup zone\n",
    "    'DK1': 'Denmark'\n",
    "}\n",
    "\n",
    "print(f\"Configuration: {len(ZONES)} zones\")\n",
    "print(f\"Countries: {', '.join(COUNTRY_MAP.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zone_data(zone_name):\n",
    "    \"\"\"\n",
    "    Load raw price data from collection step\n",
    "    \"\"\"\n",
    "    filepath = RAW_DIR / f\"{zone_name}_raw.csv\"\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        print(f\"Warning: {filepath} not found\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load all zones\n",
    "print(\"Loading raw data...\\n\")\n",
    "all_data = {}\n",
    "\n",
    "for zone in ZONES:\n",
    "    df = load_zone_data(zone)\n",
    "    if df is not None:\n",
    "        all_data[zone] = df\n",
    "        print(f\"{zone}: {len(df):,} hourly records\")\n",
    "\n",
    "print(f\"\\nLoaded: {len(all_data)}/{len(ZONES)} zones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline\n",
    "\n",
    "### Step 1: Outlier Detection (Z-Score Method)\n",
    "\n",
    "**Reference**: Chikodili et al. (2021) - Outlier Detection in Multivariate Time Series Data\n",
    "\n",
    "**Method**: \n",
    "- Calculate z-score for each price observation\n",
    "- Flag outliers: |z-score| > 3.0\n",
    "- Replace outliers with NaN for interpolation\n",
    "\n",
    "**Rationale**: \n",
    "- F-measure: 0.9978 (superior to IQR method's 0.8571)\n",
    "- Standard approach in electricity price analysis\n",
    "- Probability < 0.003 for points beyond Â±3 standard deviations\n",
    "\n",
    "### Step 2: Missing Data Imputation (Cubic Spline)\n",
    "\n",
    "**Reference**: Moritz & Bartz-Beielstein (2017) - imputeTS: Time Series Missing Value Imputation\n",
    "\n",
    "**Method**:\n",
    "- Cubic spline interpolation for gaps\n",
    "- Piecewise polynomial fitting\n",
    "\n",
    "**Rationale**:\n",
    "- Captures non-linear electricity price dynamics\n",
    "- Avoids Runge's phenomenon (oscillation in polynomial interpolation)\n",
    "- Smoother transitions than linear methods\n",
    "\n",
    "### Step 3: Normalization\n",
    "\n",
    "**Reference**: Faiq et al. (2024) - Impact of Data Normalization on Electricity Forecasting\n",
    "\n",
    "**Methods**:\n",
    "1. **Min-Max [0,1]**: For ML model inputs (LSTM optimal: CVRMSE 10.3)\n",
    "2. **Z-Score**: For relative price position analysis\n",
    "\n",
    "**Rationale**:\n",
    "- Min-max proven optimal for neural network electricity forecasting\n",
    "- Z-score useful for understanding relative market positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(series, threshold=3.0):\n",
    "    \"\"\"\n",
    "    Z-score outlier detection\n",
    "    \n",
    "    Reference: Chikodili et al. (2021)\n",
    "    Threshold: 3.0 standard deviations (F-measure 0.9978)\n",
    "    \"\"\"\n",
    "    z_scores = np.abs(zscore(series, nan_policy='omit'))\n",
    "    return z_scores > threshold\n",
    "\n",
    "\n",
    "def interpolate_spline(series):\n",
    "    \"\"\"\n",
    "    Cubic spline interpolation for missing values\n",
    "    \n",
    "    Reference: Moritz & Bartz-Beielstein (2017) - imputeTS\n",
    "    \"\"\"\n",
    "    if series.isna().sum() == 0:\n",
    "        return series\n",
    "    \n",
    "    return series.interpolate(\n",
    "        method='cubic',\n",
    "        limit_direction='both',\n",
    "        order=3\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess_zone_data(df, zone_name):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline\n",
    "    \n",
    "    Steps:\n",
    "    1. Extract date/hour from timestamp\n",
    "    2. Detect and remove outliers (z-score)\n",
    "    3. Interpolate missing values (cubic spline)\n",
    "    \n",
    "    Returns: Clean DataFrame and statistics\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Date and hour extraction\n",
    "    df_clean['date'] = df_clean['timestamp'].dt.date\n",
    "    df_clean['hour'] = df_clean['timestamp'].dt.hour\n",
    "    \n",
    "    # Step 1: Outlier detection\n",
    "    outlier_mask = detect_outliers_zscore(df_clean['price_eur_mwh'])\n",
    "    n_outliers = outlier_mask.sum()\n",
    "    \n",
    "    # Replace outliers with NaN\n",
    "    df_clean.loc[outlier_mask, 'price_eur_mwh'] = np.nan\n",
    "    \n",
    "    # Step 2: Count original missing values\n",
    "    n_missing_original = df_clean['price_eur_mwh'].isna().sum()\n",
    "    \n",
    "    # Step 3: Interpolate\n",
    "    df_clean['price_eur_mwh'] = interpolate_spline(df_clean['price_eur_mwh'])\n",
    "    \n",
    "    # Statistics\n",
    "    stats = {\n",
    "        'zone': zone_name,\n",
    "        'total_records': len(df_clean),\n",
    "        'outliers_detected': n_outliers,\n",
    "        'outlier_pct': round(100 * n_outliers / len(df_clean), 3),\n",
    "        'missing_original': n_missing_original - n_outliers,\n",
    "        'total_interpolated': n_missing_original\n",
    "    }\n",
    "    \n",
    "    return df_clean, stats\n",
    "\n",
    "\n",
    "print(\"Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "print(\"Applying preprocessing pipeline...\\n\")\n",
    "print(\"Zone | Records   | Outliers | Missing | Interpolated\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "preprocessed_data = {}\n",
    "preprocessing_stats = []\n",
    "\n",
    "for zone, df in all_data.items():\n",
    "    df_clean, stats = preprocess_zone_data(df, zone)\n",
    "    preprocessed_data[zone] = df_clean\n",
    "    preprocessing_stats.append(stats)\n",
    "    \n",
    "    # Save\n",
    "    filepath = CLEAN_DIR / f\"{zone}_clean.csv\"\n",
    "    df_clean.to_csv(filepath, index=False)\n",
    "    \n",
    "    print(f\"{zone:4s} | {stats['total_records']:9,} | {stats['outliers_detected']:8d} | \"\n",
    "          f\"{stats['missing_original']:7d} | {stats['total_interpolated']:12d}\")\n",
    "\n",
    "# Save statistics\n",
    "stats_df = pd.DataFrame(preprocessing_stats)\n",
    "stats_df.to_csv(DATA_DIR / 'preprocessing_statistics.csv', index=False)\n",
    "\n",
    "print(f\"\\nCleaned data saved to {CLEAN_DIR}\")\n",
    "print(f\"Statistics saved to preprocessing_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily Profile Creation\n",
    "\n",
    "Transform hourly data into 24-hour daily profiles with dual normalization:\n",
    "\n",
    "1. **Raw prices**: Original EUR/MWh values\n",
    "2. **Min-Max [0,1]**: For clustering and ML models\n",
    "3. **Z-Score**: For statistical analysis\n",
    "\n",
    "Each profile represents one complete day (24 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_calendar_features(date):\n",
    "    \"\"\"\n",
    "    Add temporal features for XGBoost model\n",
    "    \"\"\"\n",
    "    dt = pd.to_datetime(date)\n",
    "    \n",
    "    weekday = dt.dayofweek\n",
    "    is_weekend = weekday >= 5\n",
    "    month = dt.month\n",
    "    \n",
    "    # Meteorological seasons\n",
    "    if month in [12, 1, 2]:\n",
    "        season = 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        season = 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        season = 'Summer'\n",
    "    else:\n",
    "        season = 'Autumn'\n",
    "    \n",
    "    return {\n",
    "        'weekday': weekday,\n",
    "        'is_weekend': is_weekend,\n",
    "        'month': month,\n",
    "        'season': season\n",
    "    }\n",
    "\n",
    "\n",
    "def create_daily_profiles(df, zone_name):\n",
    "    \"\"\"\n",
    "    Create 24-hour daily profiles with triple representation:\n",
    "    - Raw prices (EUR/MWh)\n",
    "    - Min-max normalized [0,1]\n",
    "    - Z-score normalized\n",
    "    \n",
    "    Output format:\n",
    "    - Columns: h00_raw, h01_raw, ..., h23_raw (raw prices)\n",
    "    - Columns: h00_minmax, h01_minmax, ..., h23_minmax (min-max normalized)\n",
    "    - Columns: h00_zscore, h01_zscore, ..., h23_zscore (z-score normalized)\n",
    "    - Plus: daily statistics and calendar features\n",
    "    \"\"\"\n",
    "    # Pivot to 24-hour format\n",
    "    profiles = df.pivot_table(\n",
    "        index='date',\n",
    "        columns='hour',\n",
    "        values='price_eur_mwh',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Keep only complete days\n",
    "    profiles = profiles.dropna()\n",
    "    \n",
    "    # Rename to raw columns\n",
    "    raw_cols = {h: f'h{h:02d}_raw' for h in range(24)}\n",
    "    profiles = profiles.rename(columns=raw_cols)\n",
    "    \n",
    "    # Get list of raw column names\n",
    "    raw_cols_list = [f'h{h:02d}_raw' for h in range(24)]\n",
    "    daily_values = profiles[raw_cols_list].values\n",
    "    \n",
    "    # Min-max normalization (per day)\n",
    "    daily_min = daily_values.min(axis=1, keepdims=True)\n",
    "    daily_max = daily_values.max(axis=1, keepdims=True)\n",
    "    daily_range = daily_max - daily_min\n",
    "    daily_range[daily_range == 0] = 1  # Avoid division by zero\n",
    "    \n",
    "    minmax_values = (daily_values - daily_min) / daily_range\n",
    "    \n",
    "    for h in range(24):\n",
    "        profiles[f'h{h:02d}_minmax'] = minmax_values[:, h]\n",
    "    \n",
    "    # Z-score normalization (per day)\n",
    "    zscore_values = zscore(daily_values, axis=1)\n",
    "    \n",
    "    for h in range(24):\n",
    "        profiles[f'h{h:02d}_zscore'] = zscore_values[:, h]\n",
    "    \n",
    "    # Daily statistics\n",
    "    profiles['daily_mean'] = daily_values.mean(axis=1)\n",
    "    profiles['daily_std'] = daily_values.std(axis=1)\n",
    "    profiles['daily_min'] = daily_values.min(axis=1)\n",
    "    profiles['daily_max'] = daily_values.max(axis=1)\n",
    "    profiles['daily_range'] = profiles['daily_max'] - profiles['daily_min']\n",
    "    \n",
    "    # Calendar features\n",
    "    cal_features = profiles.index.map(add_calendar_features)\n",
    "    profiles['weekday'] = [f['weekday'] for f in cal_features]\n",
    "    profiles['is_weekend'] = [f['is_weekend'] for f in cal_features]\n",
    "    profiles['month'] = [f['month'] for f in cal_features]\n",
    "    profiles['season'] = [f['season'] for f in cal_features]\n",
    "    \n",
    "    # Zone metadata\n",
    "    profiles['zone'] = zone_name\n",
    "    profiles['country'] = COUNTRY_MAP[zone_name]\n",
    "    \n",
    "    return profiles\n",
    "\n",
    "\n",
    "print(\"Creating daily profiles with dual normalization...\\n\")\n",
    "all_profiles = {}\n",
    "\n",
    "for zone, df in preprocessed_data.items():\n",
    "    prof = create_daily_profiles(df, zone)\n",
    "    all_profiles[zone] = prof\n",
    "    \n",
    "    # Save as Parquet (efficient for large datasets)\n",
    "    filepath = PROFILES_DIR / f\"{zone}_profiles.parquet\"\n",
    "    prof.to_parquet(filepath)\n",
    "    \n",
    "    print(f\"{zone}: {len(prof)} complete days -> {filepath.name}\")\n",
    "\n",
    "print(f\"\\nAll profiles saved to {PROFILES_DIR}\")\n",
    "print(f\"\\nTotal complete days: {sum(len(p) for p in all_profiles.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quality_table():\n",
    "    \"\"\"\n",
    "    Generate quality metrics table for proposal/report\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for zone in ZONES:\n",
    "        if zone not in all_profiles:\n",
    "            continue\n",
    "        \n",
    "        prof = all_profiles[zone]\n",
    "        stats = next(s for s in preprocessing_stats if s['zone'] == zone)\n",
    "        \n",
    "        row = {\n",
    "            'Zone': zone,\n",
    "            'Country': COUNTRY_MAP[zone],\n",
    "            'Complete Days': len(prof),\n",
    "            'Outliers Removed': stats['outliers_detected'],\n",
    "            'Outlier Rate (%)': stats['outlier_pct'],\n",
    "            'Values Interpolated': stats['total_interpolated'],\n",
    "            'Mean Price (EUR/MWh)': round(prof['daily_mean'].mean(), 2),\n",
    "            'Std Price (EUR/MWh)': round(prof['daily_mean'].std(), 2)\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "quality_table = generate_quality_table()\n",
    "quality_table.to_csv(DATA_DIR / 'quality_metrics.csv', index=False)\n",
    "\n",
    "print(\"Quality Metrics:\")\n",
    "print(quality_table.to_string(index=False))\n",
    "print(f\"\\nSaved to: {DATA_DIR / 'quality_metrics.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Price Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "countries = ['Spain', 'Poland', 'Norway', 'Sweden', 'Denmark']\n",
    "all_prof = pd.concat([p for p in all_profiles.values()], ignore_index=False)\n",
    "\n",
    "for idx, country in enumerate(countries):\n",
    "    ax = axes[idx]\n",
    "    country_data = all_prof[all_prof['country'] == country]\n",
    "    \n",
    "    ax.hist(country_data['daily_mean'], bins=50, alpha=0.7, edgecolor='black', color='steelblue')\n",
    "    ax.axvline(country_data['daily_mean'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    ax.set_title(f'{country} (n={len(country_data)} days)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Mean Daily Price (EUR/MWh)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Daily Mean Price Distributions (After Preprocessing)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "filepath = FIGURES_DIR / 'price_distributions.png'\n",
    "plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Figure saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Normalized Profiles Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, country in enumerate(countries):\n",
    "    ax = axes[idx]\n",
    "    country_data = all_prof[all_prof['country'] == country]\n",
    "    \n",
    "    # Sample 30 random days\n",
    "    sample = country_data.sample(min(30, len(country_data)))\n",
    "    \n",
    "    minmax_cols = [f'h{h:02d}_minmax' for h in range(24)]\n",
    "    \n",
    "    # Plot individual profiles\n",
    "    for _, row in sample.iterrows():\n",
    "        ax.plot(range(24), row[minmax_cols], alpha=0.3, color='blue', linewidth=1)\n",
    "    \n",
    "    # Plot median\n",
    "    median_profile = country_data[minmax_cols].median()\n",
    "    ax.plot(range(24), median_profile, color='red', linewidth=3, label='Median')\n",
    "    \n",
    "    ax.set_title(f'{country}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Normalized Price [0,1]')\n",
    "    ax.set_ylim([-0.05, 1.05])\n",
    "    ax.set_xticks(range(0, 24, 4))\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Min-Max Normalized Daily Profiles (30 samples + median)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "filepath = FIGURES_DIR / 'normalized_profiles.png'\n",
    "plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Figure saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nZones processed: {len(all_profiles)}\")\n",
    "print(f\"Total complete days: {sum(len(p) for p in all_profiles.values())}\")\n",
    "\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  - Cleaned data: {CLEAN_DIR}\")\n",
    "print(f\"  - Daily profiles: {PROFILES_DIR}\")\n",
    "print(f\"  - Quality metrics: quality_metrics.csv\")\n",
    "print(f\"  - Figures: {FIGURES_DIR}\")\n",
    "\n",
    "print(f\"\\nReady for:\")\n",
    "print(f\"  1. Hierarchical Clustering (Ward's method)\")\n",
    "print(f\"  2. XGBoost Classification (cluster prediction)\")\n",
    "print(f\"  3. XGBoost Regression (hourly price prediction per cluster)\")\n",
    "\n",
    "print(f\"\\nData format:\")\n",
    "print(f\"  - Each profile: 24h vector\")\n",
    "print(f\"  - Normalization: Min-max [0,1] for clustering\")\n",
    "print(f\"  - Features: Calendar (weekday, season, month)\")\n",
    "print(f\"  - Raw prices: Available for final predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
