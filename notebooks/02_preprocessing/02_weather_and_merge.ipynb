{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing: Weather Aggregation & Master Dataset\n",
    "\n",
    "**Project Phase:** 2. Data Integration and Preprocessing\n",
    "**Goal:** Process the raw ERA5 NetCDF weather files and merge them with the cleaned hourly price data to create the final \"Master Dataset\" for the prediction models.\n",
    "\n",
    "### Objectives (from Project Proposal)\n",
    "1.  **Load Weather Data:** Read all monthly `.nc` files for each zone (downloaded by `01_ingestion/02_weather_era5`).\n",
    "2.  **Load Price Data:** Read the cleaned hourly price data (created by `02_preprocessing/01_cleaning_and_profiles`).\n",
    "3.  [cite_start]**Aggregate Weather:** Calculate the mean for temperature, radiation, and wind components across the zone's latitude/longitude grid[cite: 117]. Calculate the sum for precipitation.\n",
    "4.  [cite_start]**Engineer Features:** Calculate `wind_speed` from the `u10` and `v10` components[cite: 71].\n",
    "5.  [cite_start]**Merge & Sync:** Combine price and weather data into a single, hourly-timestamped DataFrame[cite: 80].\n",
    "6.  **Save:** Store the final `master_dataset_hourly.parquet` for use in the `03_analysis` (XGBoost) phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Setup Paths\n",
    "# Navigate up two levels to reach the project root\n",
    "current_dir = Path(os.getcwd())\n",
    "project_dir = current_dir.parent.parent\n",
    "\n",
    "# Input paths\n",
    "weather_raw_dir = project_dir / 'data' / 'raw' / 'weather'\n",
    "prices_clean_dir = project_dir / 'data' / 'clean'\n",
    "\n",
    "# Output path\n",
    "processed_dir = project_dir / 'data' / 'processed'\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project Directory: {project_dir}\")\n",
    "print(f\"Weather Input: {weather_raw_dir}\")\n",
    "print(f\"Price Input: {prices_clean_dir}\")\n",
    "print(f\"Output Directory: {processed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We define the same zones and timeframe used during ingestion to ensure all files are processed correctly.\n",
    "[cite_start](Note: We use **NO4** to align strictly with the project proposal [cite: 57])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeframe: 10 Years (must match 02_weather_era5.ipynb)\n",
    "YEARS = [str(year) for year in range(2015, 2025)]\n",
    "MONTHS = [f\"{month:02d}\" for month in range(1, 13)]\n",
    "\n",
    "# Zones (must match 01_prices_entsoe.ipynb and 02_weather_era5.ipynb)\n",
    "ZONES = ['ES', 'DK1', 'NO4']\n",
    "\n",
    "print(f\"Processing {len(ZONES)} zones over {len(YEARS)} years...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Merge Function\n",
    "\n",
    "This function handles the core logic:\n",
    "\n",
    "1.  **Weather Processing (xarray):** It opens all monthly `.nc` files for a zone at once (`open_mfdataset`). `xarray` handles the time concatenation automatically.\n",
    "2.  **Aggregation:** It calculates the spatial mean (`.mean(dim=['latitude', 'longitude'])`) for all variables. This flattens the 4D data (time, lat, lon, var) into a 2D time series (time, var).\n",
    "3.  **Feature Engineering:** Calculates `wind_speed` from U and V components.\n",
    "4.  **Price Loading:** Loads the cleaned hourly prices.\n",
    "5.  **Merging:** Joins the two datasets on their hourly timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_merge_zone(zone):\n",
    "    print(f\"\\nProcessing Zone: {zone}\")\n",
    "    \n",
    "    # --- 1. Process Weather Data ---\n",
    "    print(f\"  Loading raw weather files for {zone}...\")\n",
    "    \n",
    "    # Find all NetCDF files for the zone\n",
    "    file_paths = list(weather_raw_dir.glob(f'era5_{zone}_*.nc'))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(f\"  Warning: No weather files found for {zone}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Use xarray to open all files as a single dataset\n",
    "    # This automatically handles time alignment and merging\n",
    "    with xr.open_mfdataset(file_paths, combine='by_coords') as ds:\n",
    "        # Aggregate across space: calculate the mean for the entire bounding box\n",
    "        # This converts the 4D grid (time, lat, lon, var) to 2D (time, var)\n",
    "        ds_agg = ds.mean(dim=['latitude', 'longitude'])\n",
    "        \n",
    "        # Convert to a pandas DataFrame\n",
    "        weather_df = ds_agg.to_dataframe()\n",
    "\n",
    "    # Rename columns for clarity (e.g., 't2m' -> 'temp_c')\n",
    "    weather_df = weather_df.rename(columns={\n",
    "        't2m': 'temp_k',  # Temperature is in Kelvin\n",
    "        'tp': 'precipitation_m', # Total precipitation (in meters)\n",
    "        'ssrd': 'solar_radiation_j_m2', # Solar radiation (Joule / m^2)\n",
    "        'u10': 'wind_u_ms', # Wind U-component (m/s)\n",
    "        'v10': 'wind_v_ms'  # Wind V-component (m/s)\n",
    "    })\n",
    "\n",
    "    # --- 2. Feature Engineering (Weather) ---\n",
    "    \n",
    "    # Convert Temperature from Kelvin to Celsius\n",
    "    weather_df['temp_c'] = weather_df['temp_k'] - 273.15\n",
    "    \n",
    "    # Calculate Wind Speed (m/s) from U and V components\n",
    "    weather_df['wind_speed_ms'] = np.sqrt(\n",
    "        weather_df['wind_u_ms']**2 + weather_df['wind_v_ms']**2\n",
    "    )\n",
    "    \n",
    "    # Select final columns\n",
    "    weather_final = weather_df[[\n",
    "        'temp_c', \n",
    "        'precipitation_m', \n",
    "        'solar_radiation_j_m2', \n",
    "        'wind_speed_ms'\n",
    "    ]]\n",
    "    \n",
    "    # Ensure index is a clean UTC timestamp (required for merging)\n",
    "    weather_final.index.name = 'timestamp'\n",
    "    weather_final = weather_final.tz_convert(None).tz_localize('UTC')\n",
    "\n",
    "    print(f\"  Weather data aggregated: {weather_final.shape[0]} hourly records\")\n",
    "\n",
    "    # --- 3. Load Clean Price Data ---\n",
    "    print(f\"  Loading clean price data for {zone}...\")\n",
    "    price_path = prices_clean_dir / f\"{zone}_clean.csv\"\n",
    "    \n",
    "    if not price_path.exists():\n",
    "        print(f\"  Error: Clean price file not found at {price_path}. Skipping merge.\")\n",
    "        return None\n",
    "        \n",
    "    price_df = pd.read_csv(price_path)\n",
    "    price_df['timestamp'] = pd.to_datetime(price_df['timestamp'], utc=True)\n",
    "    price_df = price_df.set_index('timestamp')\n",
    "\n",
    "    # --- 4. Merge Price and Weather ---\n",
    "    print(\"  Merging price and weather data...\")\n",
    "    \n",
    "    # Merge on the timestamp index\n",
    "    master_df = pd.merge(price_df, weather_final, left_index=True, right_index=True, how='inner')\n",
    "    \n",
    "    master_df = master_df.reset_index()\n",
    "    \n",
    "    print(f\"  Merge complete. Final dataset shape: {master_df.shape}\")\n",
    "    \n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Run the pipeline for all zones and save the master datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Master Dataset Creation...\")\n",
    "\n",
    "master_datasets = {}\n",
    "\n",
    "for zone in ZONES:\n",
    "    df = process_and_merge_zone(zone)\n",
    "    \n",
    "    if df is not None:\n",
    "        master_datasets[zone] = df\n",
    "        \n",
    "        # Save the final dataset\n",
    "        save_path = processed_dir / f\"{zone}_master_hourly.parquet\"\n",
    "        df.to_parquet(save_path, index=False)\n",
    "        print(f\"  Saved master dataset to {save_path.name}\")\n",
    "\n",
    "print(\"\\n----------------------------------\")\n",
    "print(\"Master Dataset Pipeline Complete.\")\n",
    "print(f\"Output files are in: {processed_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (marbl-venv)",
   "language": "python",
   "name": "marbl-venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
