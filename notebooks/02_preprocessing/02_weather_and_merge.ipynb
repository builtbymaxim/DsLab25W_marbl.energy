{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data Processing & Masterset Creation\n",
    "\n",
    "**Project:** Electricity Price Forecasting\n",
    "**Stage:** Data Engineering (Track B & Convergence)\n",
    "\n",
    "**Objective:**\n",
    "This notebook implements the second half of the data engineering pipeline. It processes the raw gridded weather data (ERA5) and merges it with the clean electricity price data (ENTSO-E) to create the final \"Masterset\" for modeling.\n",
    "\n",
    "**Methodology:**\n",
    "1.  **Weather Track (ERA5):**\n",
    "    * Input: Raw NetCDF files (Spatial Grids).\n",
    "    * Processing: Calculate **Zonal Spatial Mean** for all variables (Temperature, Wind, Solar, Precipitation).\n",
    "    * Feature Engineering: Calculate Wind Speed magnitude from U/V components.\n",
    "2.  **Price Track (ENTSO-E):**\n",
    "    * Input: Preprocessed CSVs (Wide Format).\n",
    "    * Processing: Reshape to Long Format and synchronize Timezones (UTC).\n",
    "3.  **Convergence:**\n",
    "    * Merge both streams on an exact UTC timestamp index.\n",
    "    * Save final datasets for Clustering and Forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Suppress specific pandas warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# ==========================================\n",
    "# 1. PATH CONFIGURATION\n",
    "# ==========================================\n",
    "# Define project root relative to this notebook\n",
    "DATA_DIR = Path('../../data')\n",
    "\n",
    "# Input Paths\n",
    "RAW_WEATHER_DIR = DATA_DIR / 'raw' / 'weather'\n",
    "PRICE_CLEAN_DIR = Path('data/clean') # Local folder in this directory\n",
    "\n",
    "# Output Path\n",
    "OUTPUT_DIR = DATA_DIR / 'processed'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# 2. RUN CONFIGURATION\n",
    "# ==========================================\n",
    "# Zones to process sequentially\n",
    "ZONES_TO_PROCESS = ['DK1', 'NO2', 'ES']\n",
    "\n",
    "# Years to include in the masterset (Weather data availability)\n",
    "YEARS = [2023, 2024, 2025]\n",
    "\n",
    "print(f\"Configuration Loaded.\")\n",
    "print(f\"Input Weather: {RAW_WEATHER_DIR.resolve()}\")\n",
    "print(f\"Input Prices:  {PRICE_CLEAN_DIR.resolve()}\")\n",
    "print(f\"Output Target: {OUTPUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track B: Weather Data Processing (ERA5)\n",
    "\n",
    "This function handles the complexity of multidimensional NetCDF files.\n",
    "\n",
    "**Key Operations:**\n",
    "1.  **Spatial Aggregation:** We calculate the mean of all grid points within the file (`ds.mean`). This represents the aggregate weather condition for the entire bidding zone.\n",
    "2.  **Variable Standardization:**\n",
    "    * **Wind Speed:** Calculated from U-component and V-component vectors ($\\sqrt{u^2 + v^2}$) if pre-calculated speed is missing.\n",
    "    * **Temperature:** Converted from Kelvin to Celsius.\n",
    "    * **Precipitation:** Converted from meters to millimeters.\n",
    "    * **Solar Radiation:** Converted from Joules ($J/m^2$) to Power ($W/m^2$).\n",
    "3.  **Timezone:** Ensures the index is strict UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_era5_netcdf(zone, years):\n",
    "    print(f\"  > Processing ERA5 NetCDF for {zone}...\")\n",
    "    all_hourly_data = []\n",
    "\n",
    "    for year in years:\n",
    "        for month in range(1, 13):\n",
    "            month_str = f\"{month:02d}\"\n",
    "            \n",
    "            # Construct file paths\n",
    "            path_instant = RAW_WEATHER_DIR / f\"era5_{zone}_{year}_{month_str}_instant.nc\"\n",
    "            path_accum = RAW_WEATHER_DIR / f\"era5_{zone}_{year}_{month_str}_accum.nc\"\n",
    "            \n",
    "            # Skip if files are missing (e.g. future months)\n",
    "            if not path_instant.exists() or not path_accum.exists():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # --- A. Process Instantaneous Variables (Temp, Wind) ---\n",
    "                ds_inst = xr.open_dataset(path_instant)\n",
    "                \n",
    "                # Standardize time dimension name\n",
    "                if 'valid_time' in ds_inst.coords:\n",
    "                    ds_inst = ds_inst.rename({'valid_time': 'time'})\n",
    "                \n",
    "                # Calculate Spatial Mean (collapse lat/lon)\n",
    "                df_inst = ds_inst.mean(dim=['latitude', 'longitude']).to_dataframe().reset_index()\n",
    "                \n",
    "                # Calculate Wind Speed Magnitude if missing\n",
    "                if 'si10' not in df_inst.columns:\n",
    "                    if 'u10' in df_inst.columns and 'v10' in df_inst.columns:\n",
    "                        df_inst['wind_speed_10m'] = np.sqrt(df_inst['u10']**2 + df_inst['v10']**2)\n",
    "                    else:\n",
    "                        df_inst['wind_speed_10m'] = np.nan\n",
    "                else:\n",
    "                    df_inst['wind_speed_10m'] = df_inst['si10']\n",
    "\n",
    "                # Convert Temperature (Kelvin -> Celsius)\n",
    "                if 't2m' in df_inst.columns:\n",
    "                    df_inst['temperature_2m'] = df_inst['t2m'] - 273.15\n",
    "                \n",
    "                # Select features\n",
    "                df_inst = df_inst[['time', 'temperature_2m', 'wind_speed_10m']]\n",
    "\n",
    "                # --- B. Process Accumulated Variables (Precip, Solar) ---\n",
    "                ds_acc = xr.open_dataset(path_accum)\n",
    "                \n",
    "                if 'valid_time' in ds_acc.coords:\n",
    "                    ds_acc = ds_acc.rename({'valid_time': 'time'})\n",
    "                    \n",
    "                df_acc = ds_acc.mean(dim=['latitude', 'longitude']).to_dataframe().reset_index()\n",
    "                \n",
    "                # Convert Precipitation (m -> mm)\n",
    "                df_acc['precipitation_mm'] = df_acc['tp'] * 1000 if 'tp' in df_acc.columns else 0.0\n",
    "                \n",
    "                # Convert Solar (Joules -> Watts)\n",
    "                # Divided by 3600 because ERA5 accumulates over the hour\n",
    "                df_acc['solar_radiation_W'] = df_acc['ssrd'] / 3600 if 'ssrd' in df_acc.columns else 0.0\n",
    "                \n",
    "                df_acc = df_acc[['time', 'precipitation_mm', 'solar_radiation_W']]\n",
    "\n",
    "                # --- C. Merge Instant and Accum for this month ---\n",
    "                df_month = pd.merge(df_inst, df_acc, on='time', how='inner')\n",
    "                all_hourly_data.append(df_month)\n",
    "                \n",
    "                # Cleanup\n",
    "                ds_inst.close()\n",
    "                ds_acc.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error reading {year}-{month_str}: {e}\")\n",
    "                continue\n",
    "\n",
    "    if not all_hourly_data:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # Combine all months\n",
    "    df_weather = pd.concat(all_hourly_data, ignore_index=True)\n",
    "    \n",
    "    # Finalize Index (UTC)\n",
    "    df_weather['timestamp'] = pd.to_datetime(df_weather['time'])\n",
    "    df_weather = df_weather.set_index('timestamp').sort_index()\n",
    "    df_weather = df_weather.drop(columns=['time'])\n",
    "    \n",
    "    # Ensure UTC timezone awareness\n",
    "    if df_weather.index.tz is None:\n",
    "        df_weather.index = df_weather.index.tz_localize('UTC')\n",
    "    else:\n",
    "        df_weather.index = df_weather.index.tz_convert('UTC')\n",
    "        \n",
    "    return df_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track A: Price Data Preparation (ENTSO-E)\n",
    "\n",
    "This function prepares the price data for merging.\n",
    "\n",
    "**Key Operations:**\n",
    "1.  **Melt:** Transforms the data from Wide format (`date, h00...h23`) back to Long format (`timestamp, price`) required for machine learning.\n",
    "2.  **DST Handling:** The most critical step. We convert the Local Time (Europe/Vienna) back to UTC.\n",
    "    * *Ambiguous Times (Fall Back):* Dropped/Handled via NaT to prevent duplicates.\n",
    "    * *Non-existent Times (Spring Forward):* Dropped via NaT to prevent errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_price_data(zone):\n",
    "    path = PRICE_CLEAN_DIR / f\"{zone}_preprocessed.csv\"\n",
    "    if not path.exists():\n",
    "        print(f\"  Price file not found: {path}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    print(f\"  > Loading prices from: {path.name}\")\n",
    "    df_wide = pd.read_csv(path)\n",
    "    \n",
    "    # Melt Wide -> Long\n",
    "    df_long = df_wide.melt(id_vars=['date'], var_name='hour_str', value_name='price_eur_mwh')\n",
    "    \n",
    "    # Reconstruct timestamps (Naive first)\n",
    "    df_long['hour'] = df_long['hour_str'].str.replace('h', '').astype(int)\n",
    "    df_long['timestamp_naive'] = pd.to_datetime(df_long['date']) + pd.to_timedelta(df_long['hour'], unit='h')\n",
    "    \n",
    "    # Set index and sort\n",
    "    df_long = df_long.set_index('timestamp_naive').sort_index()\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_long = df_long[~df_long.index.duplicated(keep='first')]\n",
    "\n",
    "    # Robust DST Conversion (Local -> UTC)\n",
    "    try:\n",
    "        # Standard conversion\n",
    "        df_long.index = df_long.index.tz_localize(\n",
    "            'Europe/Vienna', \n",
    "            ambiguous='infer', \n",
    "            nonexistent='shift_forward'\n",
    "        ).tz_convert('UTC')\n",
    "        \n",
    "    except Exception:\n",
    "        # Fallback for problematic transitions\n",
    "        # We mark ambiguous/nonexistent times as NaT and drop them\n",
    "        temp_index = df_long.index.tz_localize(\n",
    "            'Europe/Vienna', \n",
    "            ambiguous='NaT', \n",
    "            nonexistent='NaT'\n",
    "        )\n",
    "        \n",
    "        # Drop rows where conversion failed\n",
    "        df_long = df_long[~temp_index.isna()]\n",
    "        df_long.index = temp_index[~temp_index.isna()].tz_convert('UTC')\n",
    "\n",
    "    return df_long[['price_eur_mwh']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Loop\n",
    "\n",
    "We iterate through all configured zones (`DK1`, `NO2`, `ES`).\n",
    "For each zone, we:\n",
    "1.  Process Weather (Track B).\n",
    "2.  Prepare Prices (Track A).\n",
    "3.  **Merge (Convergence):** Inner join on UTC timestamps. This automatically trims the 10-year price history to match the available 3-year weather history.\n",
    "4.  **Save:** Export the final `_masterset.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING BATCH PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for zone in ZONES_TO_PROCESS:\n",
    "    print(f\"\\n STARTING ZONE: {zone}\")\n",
    "    \n",
    "    # 1. Run Weather Track\n",
    "    df_weather = process_era5_netcdf(zone, YEARS)\n",
    "    if df_weather.empty:\n",
    "        print(f\"Skipping {zone}: Weather data missing.\")\n",
    "        continue\n",
    "        \n",
    "    # 2. Run Price Track\n",
    "    df_price = prepare_price_data(zone)\n",
    "    if df_price.empty:\n",
    "        print(f\"Skipping {zone}: Price data missing.\")\n",
    "        continue\n",
    "        \n",
    "    # 3. Convergence (Merge)\n",
    "    # Inner join ensures we only keep rows where we have BOTH price and weather\n",
    "    df_master = pd.merge(df_price, df_weather, left_index=True, right_index=True, how='inner')\n",
    "    \n",
    "    if df_master.empty:\n",
    "        print(f\"Warning: Merge resulted in 0 rows. Check timestamps.\")\n",
    "    else:\n",
    "        # Final Polish: Convert back to Local Time for interpretation\n",
    "        df_master.index = df_master.index.tz_convert('Europe/Vienna')\n",
    "        \n",
    "        # Save Masterset\n",
    "        output_path = OUTPUT_DIR / f\"{zone}_masterset.csv\"\n",
    "        df_master.to_csv(output_path)\n",
    "        \n",
    "        print(f\"SUCCESS: Created {output_path.name}\")\n",
    "        print(f\"Dimensions: {df_master.shape}\")\n",
    "        print(f\"Time Range: {df_master.index.min()} to {df_master.index.max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BATCH PROCESSING COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation: Market Fundamentals Check\n",
    "\n",
    "We perform a targeted visual inspection based on the primary price driver for each zone:\n",
    "* **DK1 (Wind Dominated):** Plots **Wind Speed** vs. Price.\n",
    "* **ES (Solar Dominated):** Plots **Solar Irradiance** vs. Price.\n",
    "* **NO2 (Hydro/Heating):** Plots **Temperature** vs. Price (Cold weather drives demand/prices up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def validate_masterset(zone_name):\n",
    "    path = OUTPUT_DIR / f\"{zone_name}_masterset.csv\"\n",
    "    if not path.exists():\n",
    "        print(f\"Skipping validation: {path} not found.\")\n",
    "        return\n",
    "\n",
    "    # 1. Load Data\n",
    "    print(f\"\\n=== VALIDATION REPORT FOR {zone_name} ===\")\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    \n",
    "    # Fix Timezone (UTC -> Vienna)\n",
    "    df.index = pd.to_datetime(df.index, utc=True).tz_convert('Europe/Vienna')\n",
    "    \n",
    "    # 2. Correlation Check\n",
    "    if 'price_eur_mwh' in df.columns:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if 'price_eur_mwh' in numeric_cols:\n",
    "             corr = df[numeric_cols].corr()['price_eur_mwh'].sort_values()\n",
    "             print(\"Correlations with Price:\")\n",
    "             target_cols = ['wind_speed_10m', 'solar_radiation_W', 'temperature_2m', 'precipitation_mm']\n",
    "             present_cols = [c for c in target_cols if c in corr.index]\n",
    "             print(corr[present_cols])\n",
    "    \n",
    "    # 3. Dynamic Plotting Configuration\n",
    "    # Define which variable to plot for which zone\n",
    "    plot_config = {\n",
    "        'DK1': {'col': 'wind_speed_10m', 'color': 'green',  'label': 'Wind (m/s)', 'title': 'Wind Impact'},\n",
    "        'ES':  {'col': 'solar_radiation_W', 'color': 'orange', 'label': 'Solar (W/m2)', 'title': 'Solar Impact'},\n",
    "        'NO2': {'col': 'temperature_2m', 'color': 'red',    'label': 'Temp (C)', 'title': 'Heating Demand'},\n",
    "        # Fallback for unknown zones\n",
    "        'default': {'col': 'wind_speed_10m', 'color': 'gray', 'label': 'Wind (m/s)', 'title': 'Wind Impact'}\n",
    "    }\n",
    "    \n",
    "    cfg = plot_config.get(zone_name, plot_config['default'])\n",
    "    \n",
    "    # Define time window (Summer Solstice is good for Solar, but random week is fine for others)\n",
    "    # We stick to the June window as it shows clear patterns for all\n",
    "    start_str = '2024-06-20'\n",
    "    end_str = '2024-06-23' # 3 Days\n",
    "    \n",
    "    try:\n",
    "        subset = df[start_str:end_str]\n",
    "        \n",
    "        if not subset.empty:\n",
    "            fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "            \n",
    "            # Primary Axis: Weather Variable\n",
    "            color = cfg['color']\n",
    "            ax1.set_xlabel('Time (Local)')\n",
    "            ax1.set_ylabel(cfg['label'], color=color, fontweight='bold')\n",
    "            ax1.plot(subset.index, subset[cfg['col']], color=color, linewidth=2, label=cfg['label'])\n",
    "            ax1.tick_params(axis='y', labelcolor=color)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Secondary Axis: Price\n",
    "            ax2 = ax1.twinx()\n",
    "            color = 'blue'\n",
    "            ax2.set_ylabel('Price (EUR)', color=color, fontweight='bold')\n",
    "            ax2.plot(subset.index, subset['price_eur_mwh'], color=color, linestyle='--', alpha=0.6, label='Price')\n",
    "            ax2.tick_params(axis='y', labelcolor=color)\n",
    "            \n",
    "            plt.title(f\"{zone_name}: {cfg['title']} ({start_str} to {end_str})\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            print(f\"CHECK: Observe the inverse relationship between {cfg['label']} and Price.\")\n",
    "        else:\n",
    "            print(\"Warning: No data found in the validation date range.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not plot validation ({e}).\")\n",
    "\n",
    "    # 4. Spot Check\n",
    "    try:\n",
    "        if len(df) > 0:\n",
    "            check_time = df.index[len(df)//2]\n",
    "            row = df.loc[check_time]\n",
    "            \n",
    "            print(f\"\\nSPOT CHECK ROW: {check_time}\")\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"Price (ENTSO-E): {row['price_eur_mwh']:.2f} EUR/MWh\")\n",
    "            print(f\"Temp (ERA5):     {row['temperature_2m']:.2f} C\")\n",
    "            print(f\"Wind (ERA5):     {row['wind_speed_10m']:.2f} m/s\")\n",
    "            print(\"-\" * 30)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not run spot check: {e}\")\n",
    "\n",
    "# EXECUTION LOOP\n",
    "# ------------------------------\n",
    "print(\"Starting Validation Loop...\")\n",
    "if 'ZONES_TO_PROCESS' in locals():\n",
    "    for zone in ZONES_TO_PROCESS:\n",
    "        validate_masterset(zone)\n",
    "        print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "else:\n",
    "    print(\"ZONES_TO_PROCESS list not found. Please run configuration cell.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
