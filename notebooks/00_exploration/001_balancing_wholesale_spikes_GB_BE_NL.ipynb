{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# MARBL — UK Balancing & Wholesale (with BE/NL backups) + Spike Analysis\n",
    "\n",
    "Purpose\n",
    "- **Primary:** UK Balancing (Elexon, keyless) and UK Wholesale Day-Ahead (ENTSO-E token).\n",
    "- **Backup:** Belgium Balancing (Elia Opendatasoft) + Day-Ahead (ENTSO-E), Netherlands Day-Ahead (ENTSO-E), NL Balancing stub (TenneT when key is approved).\n",
    "- **Outputs:** Idempotent CSVs, robust availability snapshot, and **spike analysis** (incl. Brexit-related windows).\n",
    "\n",
    "Tokens / Keys\n",
    "- ENTSO-E: set `ENTSOE_TOKEN` (required for day-ahead prices).\n",
    "- TenneT (NL Balancing): set `TENNET_API_KEY` (optional; when approved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switches (enable/disable data sources)\n",
    "RUN_GB_BAL = True    # Elexon system prices (balancing, 30-min)\n",
    "RUN_GB_DA  = True    # ENTSO-E Day-Ahead (GB zone)\n",
    "\n",
    "RUN_BE_BAL = True    # Elia ods133 (imbalance price, 1-min)\n",
    "RUN_BE_DA  = True    # ENTSO-E Day-Ahead (BE zone)\n",
    "\n",
    "RUN_NL_BAL = False   # TenneT (requires key; leave False until approved)\n",
    "RUN_NL_DA  = True    # ENTSO-E Day-Ahead (NL zone)\n",
    "\n",
    "# Deps\n",
    "try:\n",
    "    import requests  # noqa: F401\n",
    "except ModuleNotFoundError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"requests\", \"--quiet\"])\n",
    "    import requests  # noqa: F401\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "# IO structure (idempotent)\n",
    "DATA_DIR   = Path(\"./data\"); DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GB_BAL_DIR = DATA_DIR / \"gb_bal\"; GB_BAL_DIR.mkdir(exist_ok=True)\n",
    "GB_DA_DIR  = DATA_DIR / \"gb_da\";  GB_DA_DIR.mkdir(exist_ok=True)\n",
    "BE_BAL_DIR = DATA_DIR / \"be_bal\"; BE_BAL_DIR.mkdir(exist_ok=True)\n",
    "BE_DA_DIR  = DATA_DIR / \"be_da\";  BE_DA_DIR.mkdir(exist_ok=True)\n",
    "NL_BAL_DIR = DATA_DIR / \"nl_bal\"; NL_BAL_DIR.mkdir(exist_ok=True)\n",
    "NL_DA_DIR  = DATA_DIR / \"nl_da\";  NL_DA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# HTTP session\n",
    "import requests\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"MARBL-smoketest/4.0\"})\n",
    "DEFAULT_TIMEOUT = 45\n",
    "\n",
    "# Tokens\n",
    "ENTSOE_TOKEN = os.getenv(\"ENTSOE_TOKEN\", \"\").strip()\n",
    "TENNET_API_KEY = os.getenv(\"TENNET_API_KEY\", \"\").strip()\n",
    "TENNET_KEY_HEADER = os.getenv(\"TENNET_KEY_HEADER\", \"Ocp-Apim-Subscription-Key\")  # alt: X-API-Key\n",
    "\n",
    "def log(msg: str):\n",
    "    print(f\"[MARBL] {msg}\")\n",
    "\n",
    "print(f\"[TOKENS] ENTSOE_TOKEN set: {bool(ENTSOE_TOKEN)} | TENNET_API_KEY set: {bool(TENNET_API_KEY)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time window (UTC). Use explicit dates or LAST_N_DAYS.\n",
    "FROM_DATE = None     # e.g., \"2019-12-01\"\n",
    "TO_DATE   = None     # e.g., \"2020-02-01\"\n",
    "LAST_N_DAYS = 5      # used if FROM/TO are not set\n",
    "\n",
    "def compute_window():\n",
    "    if FROM_DATE and TO_DATE:\n",
    "        start = datetime.fromisoformat(FROM_DATE).replace(tzinfo=timezone.utc)\n",
    "        end   = datetime.fromisoformat(TO_DATE).replace(tzinfo=timezone.utc) + timedelta(days=1)\n",
    "        return start, end\n",
    "    end = datetime.now(timezone.utc).replace(second=0, micro_ok=True)\n",
    "    # Fallback: some Python versions don't accept micro_ok; enforce microsecond=0\n",
    "    try:\n",
    "        end = end\n",
    "    except Exception:\n",
    "        end = datetime.now(timezone.utc).replace(second=0, microsecond=0)\n",
    "    start = end - timedelta(days=LAST_N_DAYS)\n",
    "    return start, end\n",
    "\n",
    "WINDOW_START, WINDOW_END = compute_window()\n",
    "log(f\"Window: {WINDOW_START.isoformat()} .. {WINDOW_END.isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def http_get(url: str, params=None, headers=None, timeout=DEFAULT_TIMEOUT):\n",
    "    r = SESSION.get(url, params=params, headers=headers, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r\n",
    "\n",
    "def to_utc(ts):\n",
    "    if ts is None: return None\n",
    "    if isinstance(ts, datetime):\n",
    "        return ts if ts.tzinfo else ts.replace(tzinfo=timezone.utc)\n",
    "    try:\n",
    "        return pd.to_datetime(ts, utc=True, errors=\"coerce\").to_pydatetime()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def write_csv_no_dups(df: pd.DataFrame, path: Path, key_cols):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if df is None or df.empty: \n",
    "        return False, 0\n",
    "    df2 = df.copy()\n",
    "    for c in key_cols:\n",
    "        if c in df2.columns and \"ts\" in c:\n",
    "            df2[c] = pd.to_datetime(df2[c], utc=True, errors=\"coerce\")\n",
    "    if path.exists():\n",
    "        try:\n",
    "            old = pd.read_csv(path)\n",
    "            for c in key_cols:\n",
    "                if c in old.columns and \"ts\" in c:\n",
    "                    old[c] = pd.to_datetime(old[c], utc=True, errors=\"coerce\")\n",
    "            df2 = pd.concat([old, df2], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            log(f\"WARN: reading existing {path.name} failed: {e}\")\n",
    "    before = len(df2)\n",
    "    df2 = df2.drop_duplicates(subset=[c for c in key_cols if c in df2.columns]).sort_values(key_cols)\n",
    "    removed = before - len(df2)\n",
    "    tmp = path.with_suffix(path.suffix + \".tmp\")\n",
    "    df2.to_csv(tmp, index=False)\n",
    "    tmp.replace(path)\n",
    "    return True, removed\n",
    "\n",
    "def qc_print(df: pd.DataFrame, label: str, val_col: str | None):\n",
    "    if df is None or df.empty:\n",
    "        print(f\"{label}: empty\")\n",
    "        return\n",
    "    dups = df.duplicated(subset=[c for c in [\"ts_utc\", val_col] if c in df.columns]).sum()\n",
    "    nas  = df[val_col].isna().sum() if val_col and val_col in df.columns else \"NA\"\n",
    "    tmin = df[\"ts_utc\"].min() if \"ts_utc\" in df.columns else None\n",
    "    tmax = df[\"ts_utc\"].max() if \"ts_utc\" in df.columns else None\n",
    "    print(f\"{label}: rows={len(df)} dups={dups} na={nas} ts=[{tmin} .. {tmax}]\")\n",
    "    display(df.head(8))\n",
    "\n",
    "def scan_dir_safe(dir_path: Path, label: str, ts_candidates=(\"ts_utc\",\"start_time\",\"time\")) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for p in sorted(dir_path.glob(\"*.csv\")):\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            tcol = next((c for c in ts_candidates if c in df.columns), None)\n",
    "            if tcol:\n",
    "                df[tcol] = pd.to_datetime(df[tcol], utc=True, errors=\"coerce\")\n",
    "                rows.append({\"dataset\": label, \"file\": p.name, \"rows\": int(len(df)),\n",
    "                             \"min_ts\": df[tcol].min(), \"max_ts\": df[tcol].max()})\n",
    "            else:\n",
    "                rows.append({\"dataset\": label, \"file\": p.name, \"rows\": int(len(df)),\n",
    "                             \"min_ts\": None, \"max_ts\": None})\n",
    "        except Exception as e:\n",
    "            rows.append({\"dataset\": label, \"file\": p.name, \"rows\": None, \"min_ts\": None, \"max_ts\": None})\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GB Balancing — Elexon system prices (30-min), keyless\n",
    "ELEXON_JSON = \"https://data.elexon.co.uk/bmrs/api/v1/system-price\"\n",
    "ELEXON_CSV  = \"https://data.elexon.co.uk/bmrs/api/v1/system-price/csv\"\n",
    "\n",
    "def gb_system_prices_day(day: str) -> pd.DataFrame:\n",
    "    start = f\"{day}T00:00:00Z\"; end = f\"{day}T23:59:59Z\"\n",
    "    try:\n",
    "        r = http_get(ELEXON_JSON, params={\"from\": start, \"to\": end}, timeout=30)\n",
    "        if \"application/json\" in r.headers.get(\"content-type\",\"\").lower():\n",
    "            js = r.json()\n",
    "            items = js.get(\"data\") or js.get(\"items\") or js\n",
    "            if isinstance(items, dict): items = items.get(\"results\", [])\n",
    "            rows = []\n",
    "            for it in (items if isinstance(items, list) else []):\n",
    "                ts = it.get(\"settlementDateTime\") or it.get(\"time\") or it.get(\"timestamp\")\n",
    "                sp = it.get(\"systemPrice\") or it.get(\"system_price\") or it.get(\"price\")\n",
    "                if ts is None or sp is None: \n",
    "                    continue\n",
    "                rows.append({\"ts_utc\": to_utc(ts), \"sp_gbp_mwh\": float(sp)})\n",
    "            df = pd.DataFrame(rows).dropna(subset=[\"ts_utc\"]).sort_values(\"ts_utc\")\n",
    "            if not df.empty: return df\n",
    "    except Exception as e:\n",
    "        log(f\"GB JSON error: {e}\")\n",
    "    try:\n",
    "        r = http_get(ELEXON_CSV, params={\"from\": start, \"to\": end}, timeout=30)\n",
    "        df = pd.read_csv(io.StringIO(r.text))\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        ts_col = cols.get(\"settlementdatetime\") or cols.get(\"time\") or cols.get(\"timestamp\") or list(df.columns)[0]\n",
    "        price_col = cols.get(\"systemprice\") or cols.get(\"price\") or list(df.columns)[-1]\n",
    "        out = pd.DataFrame({\n",
    "            \"ts_utc\": pd.to_datetime(df[ts_col], utc=True, errors=\"coerce\"),\n",
    "            \"sp_gbp_mwh\": pd.to_numeric(df[price_col], errors=\"coerce\")\n",
    "        }).dropna(subset=[\"ts_utc\"]).sort_values(\"ts_utc\")\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        log(f\"GB CSV error: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTSO-E Day-Ahead (Wholesale) — requires ENTSOE_TOKEN\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "ENTSOE_BASE = \"https://web-api.tp.entsoe.eu/api\"\n",
    "BZN = {\"GB\": \"10YGB----------A\", \"BE\": \"10YBE----------2\", \"NL\": \"10YNL----------L\"}\n",
    "\n",
    "def _ti(dt: datetime) -> str:\n",
    "    return dt.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "def entsoe_da_prices(bzn: str, start_dt: datetime, end_dt: datetime) -> pd.DataFrame:\n",
    "    if not ENTSOE_TOKEN:\n",
    "        log(\"ENTSOE_TOKEN not set -> skip DA\")\n",
    "        return pd.DataFrame()\n",
    "    params = {\n",
    "        \"securityToken\": ENTSOE_TOKEN, \"documentType\": \"A44\",\n",
    "        \"in_Domain\": bzn, \"out_Domain\": bzn,\n",
    "        \"timeInterval\": f\"{_ti(start_dt)}-{_ti(end_dt)}\",\n",
    "    }\n",
    "    r = http_get(ENTSOE_BASE, params=params, timeout=60)\n",
    "    root = ET.fromstring(r.text)\n",
    "    ns = {\"ns\": root.tag.split('}')[0].strip('{')} if '}' in root.tag else {}\n",
    "    rows = []\n",
    "    for ts in (root.findall(\".//ns:TimeSeries\", ns) if ns else root.findall(\".//TimeSeries\")):\n",
    "        for period in (ts.findall(\".//ns:Period\", ns) if ns else ts.findall(\".//Period\")):\n",
    "            start_str = period.findtext(\"./ns:timeInterval/ns:start\", default=\"\", namespaces=ns) if ns else period.findtext(\"./timeInterval/start\", \"\")\n",
    "            start_ts = pd.to_datetime(start_str, utc=True, errors=\"coerce\")\n",
    "            for point in (period.findall(\"./ns:Point\", ns) if ns else period.findall(\"./Point\")):\n",
    "                pos  = point.findtext(\"./ns:position\", default=\"\", namespaces=ns) if ns else point.findtext(\"./position\", \"\")\n",
    "                pval = point.findtext(\"./ns:price.amount\", default=\"\", namespaces=ns) if ns else point.findtext(\"./price.amount\", \"\")\n",
    "                if not start_ts or not pval: continue\n",
    "                try: k = int(pos) - 1\n",
    "                except Exception: k = 0\n",
    "                ts_utc = start_ts + timedelta(hours=k)\n",
    "                rows.append({\"ts_utc\": ts_utc, \"da_price\": pd.to_numeric(pval, errors=\"coerce\")})\n",
    "    return pd.DataFrame(rows).dropna(subset=[\"ts_utc\"]).sort_values(\"ts_utc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BE Balancing — Elia ods133 (fast: v2.1 where + CSV fallback + daily chunking)\n",
    "ELIA_V21 = \"https://opendata.elia.be/api/explore/v2.1/catalog/datasets/ods133/records\"\n",
    "ELIA_CSV = \"https://opendata.elia.be/explore/dataset/ods133/download/\"\n",
    "ELIA_EARLIEST = datetime(2024, 5, 22, tzinfo=timezone.utc)\n",
    "\n",
    "def _isoz(dt: datetime) -> str:\n",
    "    return dt.astimezone(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def be_ods133_v21_range(start_dt: datetime, end_dt: datetime, page_rows=10000, timeout=60) -> pd.DataFrame:\n",
    "    start_dt = max(start_dt, ELIA_EARLIEST)\n",
    "    if start_dt >= end_dt: return pd.DataFrame(columns=[\"ts_utc\",\"price_eur_mwh\"])\n",
    "    rows, offset = [], 0\n",
    "    where = f'datetime >= \"{_isoz(start_dt)}\" AND datetime < \"{_isoz(end_dt)}\"'\n",
    "    while True:\n",
    "        r = http_get(ELIA_V21, params={\"where\": where, \"order_by\": \"datetime\", \"limit\": page_rows, \"offset\": offset, \"timezone\":\"UTC\"}, timeout=timeout)\n",
    "        js = r.json(); batch = js.get(\"results\", [])\n",
    "        if not batch: break\n",
    "        for it in batch:\n",
    "            ts = it.get(\"datetime\"); px = it.get(\"imbalance_price_eur_per_mwh\")\n",
    "            if ts is None or px is None: continue\n",
    "            rows.append({\"ts_utc\": pd.to_datetime(ts, utc=True, errors=\"coerce\"), \"price_eur_mwh\": pd.to_numeric(px, errors=\"coerce\")})\n",
    "        if len(batch) < page_rows: break\n",
    "        offset += page_rows\n",
    "        if offset >= 1_000_000: break\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty: return df\n",
    "    return df.dropna(subset=[\"ts_utc\"]).drop_duplicates(subset=[\"ts_utc\"]).sort_values(\"ts_utc\").reset_index(drop=True)\n",
    "\n",
    "def be_ods133_csv_range(start_dt: datetime, end_dt: datetime, timeout=90) -> pd.DataFrame:\n",
    "    start_dt = max(start_dt, ELIA_EARLIEST)\n",
    "    if start_dt >= end_dt: return pd.DataFrame(columns=[\"ts_utc\",\"price_eur_mwh\"])\n",
    "    params = {\"format\":\"csv\",\"timezone\":\"UTC\",\"use_labels_for_header\":\"true\",\"rows\":\"-1\",\"where\": f'datetime >= \"{_isoz(start_dt)}\" AND datetime < \"{_isoz(end_dt)}\"'}\n",
    "    r = http_get(ELIA_CSV, params=params, timeout=timeout)\n",
    "    raw = pd.read_csv(io.StringIO(r.text))\n",
    "    cols = {c.lower(): c for c in raw.columns}\n",
    "    ts_col = cols.get(\"datetime\") or cols.get(\"date_time\") or list(raw.columns)[0]\n",
    "    price_col = cols.get(\"imbalance price eur per mwh\") or cols.get(\"price_eur_mwh\") or list(raw.columns)[-1]\n",
    "    out = pd.DataFrame({\"ts_utc\": pd.to_datetime(raw[ts_col], utc=True, errors=\"coerce\"),\n",
    "                        \"price_eur_mwh\": pd.to_numeric(raw[price_col], errors=\"coerce\")}).dropna(subset=[\"ts_utc\"]).sort_values(\"ts_utc\")\n",
    "    return out\n",
    "\n",
    "def be_ods133_between_fast(start_dt: datetime, end_dt: datetime, chunk=\"D\") -> pd.DataFrame:\n",
    "    cur_start = max(start_dt, ELIA_EARLIEST); cur_end = end_dt\n",
    "    if cur_start >= cur_end: return pd.DataFrame(columns=[\"ts_utc\",\"price_eur_mwh\"])\n",
    "    frames = []\n",
    "    edges = pd.date_range(pd.Timestamp(cur_start).floor(chunk), pd.Timestamp(cur_end).ceil(chunk), freq=chunk, tz=\"UTC\")\n",
    "    if len(edges) == 0 or edges[0] != pd.Timestamp(cur_start).floor(chunk):\n",
    "        edges = edges.insert(0, pd.Timestamp(cur_start).floor(chunk))\n",
    "    for i in range(len(edges)-1):\n",
    "        s = max(pd.Timestamp(cur_start), edges[i]).to_pydatetime()\n",
    "        e = min(pd.Timestamp(cur_end),   edges[i+1]).to_pydatetime()\n",
    "        if s >= e: continue\n",
    "        try:\n",
    "            df = be_ods133_v21_range(s, e)\n",
    "        except Exception as ex:\n",
    "            log(f\"[BE v2.1 chunk fail] {s}..{e}: {ex} -> CSV fallback\")\n",
    "            df = be_ods133_csv_range(s, e)\n",
    "        if not df.empty: frames.append(df)\n",
    "    if not frames: return pd.DataFrame(columns=[\"ts_utc\",\"price_eur_mwh\"])\n",
    "    out = pd.concat(frames, ignore_index=True).drop_duplicates(subset=[\"ts_utc\"]).sort_values(\"ts_utc\").reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NL Balancing — TenneT readiness stub\n",
    "TENNET_BASE = \"https://developer.tennet.eu\"\n",
    "def tennet_headers():\n",
    "    if not TENNET_API_KEY: return {}\n",
    "    return {TENNET_KEY_HEADER: TENNET_API_KEY, \"Accept\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHOLESALE runs (DA via ENTSO-E)\n",
    "if RUN_GB_DA:\n",
    "    if ENTSOE_TOKEN:\n",
    "        gb_da_win = entsoe_da_prices(\"10YGB----------A\", WINDOW_START, WINDOW_END)\n",
    "        qc_print(gb_da_win, \"GB — ENTSOE DA (window)\", \"da_price\")\n",
    "        if not gb_da_win.empty:\n",
    "            stamp = f\"{WINDOW_START:%Y%m%d%H%M}_{WINDOW_END:%Y%m%d%H%M}\"\n",
    "            write_csv_no_dups(gb_da_win, GB_DA_DIR / f\"GB_DA_window_{stamp}.csv\", key_cols=[\"ts_utc\",\"da_price\"])\n",
    "    else:\n",
    "        log(\"GB DA skipped (ENTSOE_TOKEN missing).\")\n",
    "\n",
    "if RUN_BE_DA:\n",
    "    if ENTSOE_TOKEN:\n",
    "        be_da_win = entsoe_da_prices(\"10YBE----------2\", WINDOW_START, WINDOW_END)\n",
    "        qc_print(be_da_win, \"BE — ENTSOE DA (window)\", \"da_price\")\n",
    "        if not be_da_win.empty:\n",
    "            stamp = f\"{WINDOW_START:%Y%m%d%H%M}_{WINDOW_END:%Y%m%d%H%M}\"\n",
    "            write_csv_no_dups(be_da_win, BE_DA_DIR / f\"BE_DA_window_{stamp}.csv\", key_cols=[\"ts_utc\",\"da_price\"])\n",
    "    else:\n",
    "        log(\"BE DA skipped (ENTSOE_TOKEN missing).\")\n",
    "\n",
    "if RUN_NL_DA:\n",
    "    if ENTSOE_TOKEN:\n",
    "        nl_da_win = entsoe_da_prices(\"10YNL----------L\", WINDOW_START, WINDOW_END)\n",
    "        qc_print(nl_da_win, \"NL — ENTSOE DA (window)\", \"da_price\")\n",
    "        if not nl_da_win.empty:\n",
    "            stamp = f\"{WINDOW_START:%Y%m%d%H%M}_{WINDOW_END:%Y%m%d%H%M}\"\n",
    "            write_csv_no_dups(nl_da_win, NL_DA_DIR / f\"NL_DA_window_{stamp}.csv\", key_cols=[\"ts_utc\",\"da_price\"])\n",
    "    else:\n",
    "        log(\"NL DA skipped (ENTSOE_TOKEN missing).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BALANCING runs\n",
    "if RUN_GB_BAL:\n",
    "    yday = (datetime.now(timezone.utc) - timedelta(days=1)).date().strftime(\"%Y-%m-%d\")\n",
    "    gb_bal_day = gb_system_prices_day(yday)\n",
    "    qc_print(gb_bal_day, \"GB — Elexon Balancing (yday)\", \"sp_gbp_mwh\")\n",
    "    if not gb_bal_day.empty:\n",
    "        write_csv_no_dups(gb_bal_day, GB_BAL_DIR / f\"GB_bal_day_{yday.replace('-','')}.csv\",\n",
    "                          key_cols=[\"ts_utc\",\"sp_gbp_mwh\"])\n",
    "else:\n",
    "    log(\"GB Balancing disabled.\")\n",
    "\n",
    "if RUN_BE_BAL:\n",
    "    be_bal_win = be_ods133_between_fast(WINDOW_START, WINDOW_END, chunk=\"D\")\n",
    "    qc_print(be_bal_win, \"BE — Elia ods133 (window, fast)\", \"price_eur_mwh\")\n",
    "    if not be_bal_win.empty:\n",
    "        stamp = f\"{WINDOW_START:%Y%m%d%H%M}_{WINDOW_END:%Y%m%d%H%M}\"\n",
    "        write_csv_no_dups(be_bal_win, BE_BAL_DIR / f\"BE_bal_window_{stamp}.csv\",\n",
    "                          key_cols=[\"ts_utc\",\"price_eur_mwh\"])\n",
    "else:\n",
    "    log(\"BE Balancing disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Availability snapshot\n",
    "frames = []\n",
    "frames.append(scan_dir_safe(GB_DA_DIR,  \"GB_DA\"))\n",
    "frames.append(scan_dir_safe(GB_BAL_DIR, \"GB_BAL\"))\n",
    "frames.append(scan_dir_safe(BE_DA_DIR,  \"BE_DA\"))\n",
    "frames.append(scan_dir_safe(BE_BAL_DIR, \"BE_BAL\"))\n",
    "frames.append(scan_dir_safe(NL_DA_DIR,  \"NL_DA\"))\n",
    "frames.append(scan_dir_safe(NL_BAL_DIR, \"NL_BAL\"))\n",
    "avail = pd.concat([f for f in frames if f is not None], ignore_index=True) if frames else pd.DataFrame()\n",
    "avail = avail.sort_values([\"dataset\",\"file\"]).reset_index(drop=True)\n",
    "display(avail)\n",
    "\n",
    "def coverage_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"dataset\",\"files\",\"total_rows\",\"min_ts\",\"max_ts\"])\n",
    "    g = (df.groupby(\"dataset\")\n",
    "           .agg(files=(\"file\",\"count\"),\n",
    "                total_rows=(\"rows\",\"sum\"),\n",
    "                min_ts=(\"min_ts\",\"min\"),\n",
    "                max_ts=(\"max_ts\",\"max\"))\n",
    "           .reset_index())\n",
    "    return g\n",
    "\n",
    "cov = coverage_summary(avail)\n",
    "display(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spike detection utilities\n",
    "def load_all(dir_path: Path, ts_col: str, val_col: str) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for p in sorted(dir_path.glob(\"*.csv\")):\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            if ts_col not in df.columns or val_col not in df.columns: \n",
    "                continue\n",
    "            df[ts_col] = pd.to_datetime(df[ts_col], utc=True, errors=\"coerce\")\n",
    "            frames.append(df[[ts_col, val_col]])\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not frames: return pd.DataFrame(columns=[ts_col, val_col])\n",
    "    out = (pd.concat(frames, ignore_index=True)\n",
    "             .dropna()\n",
    "             .drop_duplicates(subset=[ts_col])\n",
    "             .sort_values(ts_col))\n",
    "    return out\n",
    "\n",
    "def detect_spikes(df: pd.DataFrame, ts_col: str, val_col: str, baseline_days=90, q=0.99):\n",
    "    if df.empty:\n",
    "        return df.assign(is_spike=pd.Series(dtype=bool))\n",
    "    s = df[[ts_col, val_col]].copy().sort_values(ts_col).reset_index(drop=True)\n",
    "    s[\"thr\"] = s[val_col].rolling(f\"{baseline_days}D\", on=ts_col).quantile(0.99, interpolation=\"nearest\")\n",
    "    s[\"is_spike\"] = s[val_col] > s[\"thr\"]\n",
    "    return s\n",
    "\n",
    "def window_stats(spike_df: pd.DataFrame, ts_col: str, val_col: str, start_dt: datetime, end_dt: datetime):\n",
    "    if spike_df.empty:\n",
    "        return {\"rows\": 0, \"spikes_win\": 0, \"spikes_out\": 0, \"ratio\": None}\n",
    "    m_win = (spike_df[ts_col] >= start_dt) & (spike_df[ts_col] < end_dt)\n",
    "    spikes_win = int(spike_df.loc[m_win, \"is_spike\"].sum())\n",
    "    spikes_out = int(spike_df.loc[~m_win, \"is_spike\"].sum())\n",
    "    rows = int(len(spike_df))\n",
    "    ratio = (spikes_win / max(1, spikes_out)) if spikes_out else None\n",
    "    return {\"rows\": rows, \"spikes_win\": spikes_win, \"spikes_out\": spikes_out, \"ratio\": ratio}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event windows (incl. Brexit) & analysis\n",
    "EVENTS = [\n",
    "    {\"name\": \"Brexit Referendum\", \"start\": \"2016-06-20\", \"end\": \"2016-06-27\"},\n",
    "    {\"name\": \"Brexit Day\",        \"start\": \"2020-01-29\", \"end\": \"2020-02-03\"},\n",
    "    {\"name\": \"Transition End\",    \"start\": \"2020-12-28\", \"end\": \"2021-01-03\"},\n",
    "    {\"name\": \"Gas Crisis 2021\",   \"start\": \"2021-09-01\", \"end\": \"2021-10-01\"},\n",
    "]\n",
    "\n",
    "def to_utc_dt(s: str) -> datetime:\n",
    "    return datetime.fromisoformat(s).replace(tzinfo=timezone.utc)\n",
    "\n",
    "gb_bal_all = load_all(GB_BAL_DIR, \"ts_utc\", \"sp_gbp_mwh\")\n",
    "gb_da_all  = load_all(GB_DA_DIR,  \"ts_utc\", \"da_price\")\n",
    "be_bal_all = load_all(BE_BAL_DIR, \"ts_utc\", \"price_eur_mwh\")\n",
    "be_da_all  = load_all(BE_DA_DIR,  \"ts_utc\", \"da_price\")\n",
    "nl_da_all  = load_all(NL_DA_DIR,  \"ts_utc\", \"da_price\")\n",
    "\n",
    "gb_bal_sp = detect_spikes(gb_bal_all, \"ts_utc\", \"sp_gbp_mwh\", baseline_days=90, q=0.99)\n",
    "gb_da_sp  = detect_spikes(gb_da_all,  \"ts_utc\", \"da_price\",    baseline_days=90, q=0.99)\n",
    "be_bal_sp = detect_spikes(be_bal_all, \"ts_utc\", \"price_eur_mwh\", baseline_days=90, q=0.99)\n",
    "be_da_sp  = detect_spikes(be_da_all,  \"ts_utc\", \"da_price\",    baseline_days=90, q=0.99)\n",
    "nl_da_sp  = detect_spikes(nl_da_all,  \"ts_utc\", \"da_price\",    baseline_days=90, q=0.99)\n",
    "\n",
    "rows = []\n",
    "for ev in EVENTS:\n",
    "    sdt, edt = to_utc_dt(ev[\"start\"]), to_utc_dt(ev[\"end\"])\n",
    "    def rec(ds_name, stat):\n",
    "        row = {\"event\": ev[\"name\"], \"dataset\": ds_name}\n",
    "        row.update(stat); rows.append(row)\n",
    "    if not gb_bal_sp.empty: rec(\"GB_BAL\", window_stats(gb_bal_sp, \"ts_utc\", \"sp_gbp_mwh\", sdt, edt))\n",
    "    if not gb_da_sp.empty:  rec(\"GB_DA\",  window_stats(gb_da_sp,  \"ts_utc\", \"da_price\",    sdt, edt))\n",
    "    if not be_bal_sp.empty: rec(\"BE_BAL\", window_stats(be_bal_sp, \"ts_utc\", \"price_eur_mwh\", sdt, edt))\n",
    "    if not be_da_sp.empty:  rec(\"BE_DA\",  window_stats(be_da_sp,  \"ts_utc\", \"da_price\",    sdt, edt))\n",
    "    if not nl_da_sp.empty:  rec(\"NL_DA\",  window_stats(nl_da_sp,  \"ts_utc\", \"da_price\",    sdt, edt))\n",
    "\n",
    "event_summary = pd.DataFrame(rows)\n",
    "display(event_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final narrative summary\n",
    "lines = []\n",
    "def add(s): lines.append(s); print(s)\n",
    "\n",
    "add(\"Summary — Data readiness & spikes\")\n",
    "add(f\"- ENTSO-E token set: {bool(ENTSOE_TOKEN)}.\")\n",
    "add(\"- UK Balancing (Elexon) is keyless; BE Balancing (Elia) is open; NL Balancing awaits TenneT key.\")\n",
    "add(\"- Availability and coverage are above; DA 'how far back' appears after backfilling months.\")\n",
    "\n",
    "if 'event_summary' in globals() and not event_summary.empty:\n",
    "    add(\"Event spike ratios (inside vs outside window) shown above per dataset; ratio>1 implies elevated spikes near the event.\")\n",
    "else:\n",
    "    add(\"No event-window results yet — ensure DA and/or Balancing data exist for the chosen periods.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DsLab25W_marbl (venv)",
   "name": "DsLab25W_marbl"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
