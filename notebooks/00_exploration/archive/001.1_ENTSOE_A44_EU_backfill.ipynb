{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENTSO-E Day-Ahead Price Data Collection\n",
    "\n",
    "**Purpose**: Collect 2 years of hourly day-ahead electricity prices for clustering and ML prediction\n",
    "\n",
    "**Scope**: Largest bidding zones in selected European countries\n",
    "\n",
    "## Selected Zones (Largest per Country)\n",
    "\n",
    "| Country | Zone | Code | Description |\n",
    "|---------|------|------|-------------|\n",
    "| Spain | ES | 10YES-REE------0 | Spain (single zone) |\n",
    "| Norway | NO2 | 10YNO-2--------T | Southwest Norway (largest) |\n",
    "| Denmark | DK1 | 10YDK-1--------W | West Denmark (largest) |\n",
    "\n",
    "**Note**: UK (GB) excluded as it is not part of ENTSO-E Transparency Platform since Brexit\n",
    "\n",
    "## Data Requirements for Project\n",
    "\n",
    "- **Clustering**: Requires sufficient days for pattern detection (minimum 1-2 years)\n",
    "- **XGBoost Training**: Needs seasonal variation and diverse market conditions\n",
    "- **Time Period**: 10 years (3650 days) to capture all seasonal patterns\n",
    "\n",
    "## Output Format\n",
    "\n",
    "Each zone produces:\n",
    "- `{ZONE}_raw.csv`: Hourly prices with timestamps\n",
    "- Ready for preprocessing (outlier detection, interpolation, normalization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration\n",
    "ENTSOE_TOKEN = \"7273b73e-d731-4c3d-a9eb-101bcf4ab674\"  # token\n",
    "\n",
    "# Largest bidding zones per country\n",
    "ZONES = {\n",
    "    'ES': '10YES-REE------0',   # Spain\n",
    "    'NO2': '10YNO-2--------T',  # Norway Zone 2 (Southern Norway, major volume)\n",
    "    'DK1': '10YDK-1--------W'   # Denmark West (largest)\n",
    "}\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = Path('data')\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "RAW_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Time period: 1ß years for robust clustering\n",
    "DAYS_BACK = 3650  # 10 years\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Zones: 4\")\n",
    "print(f\"  Period: {DAYS_BACK} days (~10 years)\")\n",
    "print(f\"  Output: {RAW_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Functions\n",
    "\n",
    "Following best practices from academic literature on electricity price data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_entsoe_prices(zone_code, start_date, end_date, token, max_retries=3):\n",
    "    \"\"\"\n",
    "    Fetch day-ahead electricity prices from ENTSO-E Transparency Platform\n",
    "    \n",
    "    Document Type A44: Price Document (Day-ahead prices)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with columns: timestamp (UTC), price_eur_mwh\n",
    "    \"\"\"\n",
    "    url = 'https://web-api.tp.entsoe.eu/api'\n",
    "    \n",
    "    params = {\n",
    "        'securityToken': token,\n",
    "        'documentType': 'A44',  # Day-ahead prices\n",
    "        'in_Domain': zone_code,\n",
    "        'out_Domain': zone_code,\n",
    "        'periodStart': start_date.strftime('%Y%m%d%H%M'),\n",
    "        'periodEnd': end_date.strftime('%Y%m%d%H%M')\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                root = ET.fromstring(response.text)\n",
    "                ns = {'ns': 'urn:iec62325.351:tc57wg16:451-3:publicationdocument:7:3'}\n",
    "                \n",
    "                data = []\n",
    "                \n",
    "                for timeseries in root.findall('.//ns:TimeSeries', ns):\n",
    "                    for period in timeseries.findall('.//ns:Period', ns):\n",
    "                        start = period.find('.//ns:timeInterval/ns:start', ns).text\n",
    "                        start_dt = pd.to_datetime(start, utc=True)\n",
    "                        \n",
    "                        for point in period.findall('.//ns:Point', ns):\n",
    "                            position = int(point.find('.//ns:position', ns).text)\n",
    "                            price = float(point.find('.//ns:price.amount', ns).text)\n",
    "                            \n",
    "                            timestamp = start_dt + timedelta(hours=position - 1)\n",
    "                            \n",
    "                            data.append({\n",
    "                                'timestamp': timestamp,\n",
    "                                'price_eur_mwh': price\n",
    "                            })\n",
    "                \n",
    "                if data:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "                    return df\n",
    "                else:\n",
    "                    return pd.DataFrame()\n",
    "                    \n",
    "            elif response.status_code == 429:\n",
    "                wait_time = 60 * (attempt + 1)\n",
    "                print(f\"    Rate limit hit, waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"    Error {response.status_code}: {response.text[:200]}\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(10)\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def fetch_in_chunks(zone_code, start_date, end_date, token, chunk_days=90):\n",
    "    \"\"\"\n",
    "    Fetch data in chunks to avoid API limits\n",
    "    ENTSO-E has limits on request size, so we split into 90-day chunks\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    current_start = start_date\n",
    "    \n",
    "    while current_start < end_date:\n",
    "        current_end = min(current_start + timedelta(days=chunk_days), end_date)\n",
    "        \n",
    "        print(f\"    Fetching {current_start.date()} to {current_end.date()}\")\n",
    "        \n",
    "        chunk_data = fetch_entsoe_prices(zone_code, current_start, current_end, token)\n",
    "        \n",
    "        if not chunk_data.empty:\n",
    "            all_data.append(chunk_data)\n",
    "        \n",
    "        current_start = current_end\n",
    "        time.sleep(2)  # Rate limiting between chunks\n",
    "    \n",
    "    if all_data:\n",
    "        combined = pd.concat(all_data, ignore_index=True)\n",
    "        combined = combined.sort_values('timestamp').reset_index(drop=True)\n",
    "        # Remove duplicates\n",
    "        combined = combined.drop_duplicates(subset=['timestamp'], keep='first')\n",
    "        return combined\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "print(\"Data collection functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection Process\n",
    "\n",
    "Collecting 2 years of data for each zone\n",
    "\n",
    "**Expected duration**: ~5-10 minutes (due to API rate limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date range: 2 years back from today\n",
    "end_date = datetime.utcnow()\n",
    "start_date = end_date - timedelta(days=DAYS_BACK)\n",
    "\n",
    "print(f\"Collection Period: {start_date.date()} to {end_date.date()}\")\n",
    "print(f\"Total: {DAYS_BACK} days\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Collect data for each zone\n",
    "collection_stats = []\n",
    "\n",
    "for zone_name, zone_code in ZONES.items():\n",
    "    print(f\"\\n{zone_name}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Fetch data\n",
    "    df = fetch_in_chunks(zone_code, start_date, end_date, ENTSOE_TOKEN)\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Basic statistics\n",
    "        n_records = len(df)\n",
    "        date_range = (df['timestamp'].min(), df['timestamp'].max())\n",
    "        expected_hours = (date_range[1] - date_range[0]).total_seconds() / 3600\n",
    "        completeness = 100 * n_records / expected_hours if expected_hours > 0 else 0\n",
    "        \n",
    "        # Save raw data\n",
    "        filepath = RAW_DIR / f\"{zone_name}_raw.csv\"\n",
    "        df.to_csv(filepath, index=False)\n",
    "        \n",
    "        stats = {\n",
    "            'zone': zone_name,\n",
    "            'records': n_records,\n",
    "            'start': date_range[0],\n",
    "            'end': date_range[1],\n",
    "            'completeness_pct': round(completeness, 1),\n",
    "            'mean_price': round(df['price_eur_mwh'].mean(), 2),\n",
    "            'std_price': round(df['price_eur_mwh'].std(), 2)\n",
    "        }\n",
    "        collection_stats.append(stats)\n",
    "        \n",
    "        print(f\"  Records: {n_records:,}\")\n",
    "        print(f\"  Date range: {date_range[0].date()} to {date_range[1].date()}\")\n",
    "        print(f\"  Completeness: {completeness:.1f}%\")\n",
    "        print(f\"  Mean price: {df['price_eur_mwh'].mean():.2f} EUR/MWh\")\n",
    "        print(f\"  Saved to: {filepath.name}\")\n",
    "    else:\n",
    "        print(f\"  FAILED: No data retrieved\")\n",
    "        collection_stats.append({\n",
    "            'zone': zone_name,\n",
    "            'records': 0,\n",
    "            'status': 'FAILED'\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COLLECTION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_df = pd.DataFrame(collection_stats)\n",
    "\n",
    "print(\"\\nCollection Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(DATA_DIR / 'collection_summary.csv', index=False)\n",
    "print(f\"\\nSummary saved to: {DATA_DIR / 'collection_summary.csv'}\")\n",
    "\n",
    "# Check completeness\n",
    "successful = summary_df[summary_df['records'] > 0]\n",
    "print(f\"\\nSuccessful zones: {len(successful)}/{len(ZONES)}\")\n",
    "\n",
    "if len(successful) > 0:\n",
    "    avg_completeness = successful['completeness_pct'].mean()\n",
    "    print(f\"Average completeness: {avg_completeness:.1f}%\")\n",
    "    \n",
    "    if avg_completeness < 95:\n",
    "        print(\"\\nWarning: Low data completeness detected\")\n",
    "        print(\"Some gaps will need interpolation in preprocessing step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Load collected data for quick visualization\n",
    "fig, axes = plt.subplots(len(successful), 1, figsize=(14, 3*len(successful)))\n",
    "if len(successful) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (_, row) in enumerate(successful.iterrows()):\n",
    "    zone = row['zone']\n",
    "    filepath = RAW_DIR / f\"{zone}_raw.csv\"\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    axes[idx].plot(df['timestamp'], df['price_eur_mwh'], linewidth=0.5, alpha=0.7)\n",
    "    axes[idx].set_title(f'{zone}: Raw Price Data (2 years)', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Price (EUR/MWh)')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'raw_price_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Price overview saved to: {DATA_DIR / 'raw_price_overview.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### 1. Data Preprocessing (Notebook 02)\n",
    "\n",
    "Apply validated preprocessing methods:\n",
    "- **Outlier Detection**: Z-score method (threshold=3) - Chikodili et al. (2021)\n",
    "- **Missing Data**: Cubic spline interpolation - Moritz & Bartz-Beielstein (2017)\n",
    "- **Normalization**: Min-max scaling [0,1] - Faiq et al. (2024)\n",
    "\n",
    "### 2. Daily Profile Creation\n",
    "\n",
    "Transform hourly data into 24-hour daily profiles:\n",
    "- Each day represented as vector of 24 hourly prices\n",
    "- Normalized profiles for clustering\n",
    "- Raw profiles for final predictions\n",
    "\n",
    "### 3. Hierarchical Clustering\n",
    "\n",
    "Following Roberts & Brown (2020) methodology:\n",
    "- Ward's linkage method for clustering\n",
    "- Elbow method for optimal k\n",
    "- Cluster validation and interpretation\n",
    "\n",
    "### 4. XGBoost Prediction Model\n",
    "\n",
    "Two-stage approach:\n",
    "1. **Classification**: Predict cluster for day t+1\n",
    "2. **Regression**: Cluster-specific models for hourly prices\n",
    "\n",
    "### Data Requirements Satisfied\n",
    "\n",
    "✓ **Clustering**: 2 years data provides ~730 days per zone\n",
    "✓ **Seasonal Coverage**: All seasons, weekdays, holidays captured\n",
    "✓ **XGBoost Training**: Sufficient samples for robust model training\n",
    "✓ **Cross-Validation**: Enough data for train/test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DATA COLLECTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOutput directory: {RAW_DIR}\")\n",
    "print(f\"\\nCollected zones ({len(successful)}/{len(ZONES)}):\")\n",
    "for _, row in successful.iterrows():\n",
    "    print(f\"  - {row['zone']}: {row['records']:,} records ({row['completeness_pct']:.1f}% complete)\")\n",
    "\n",
    "print(f\"\\nTotal hourly records: {successful['records'].sum():,}\")\n",
    "print(f\"\\nReady for preprocessing pipeline (Notebook 02)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DsLab25W_marbl (venv)",
   "language": "python",
   "name": "dslab25w_marbl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
